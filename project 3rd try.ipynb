{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a6b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset as HFDataset # Renamed to avoid conflict with surprise.Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import random #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d239a7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for models: cuda\n",
      "\n",
      "--- Phase 1: Data Loading, Preprocessing, and Splitting ---\n",
      "Original dataset loaded and preprocessed with 10000 samples.\n",
      "Splitting dataset into 80/20 train-test sets...\n",
      "Training dataset shape: (8000, 14)\n",
      "Testing dataset shape: (2000, 14)\n",
      "Data splitting complete. Proceeding with model training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# --- Utility Function for reproducibility ---\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device for models: {device}\")\n",
    "\n",
    "# 1. Data Aggregation and Preprocessing\n",
    "print(\"\\n--- Phase 1: Data Loading, Preprocessing, and Splitting ---\")\n",
    "df = pd.read_csv('travel_recommendations_10k.csv')\n",
    "\n",
    "# Handle missing values\n",
    "numerical_columns = ['age', 'travel_rating']\n",
    "categorical_columns = ['gender', 'hobby', 'budget', 'travel_style', 'activities', 'climate']\n",
    "\n",
    "imputer_num = SimpleImputer(strategy='mean')\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "df[numerical_columns] = imputer_num.fit_transform(df[numerical_columns])\n",
    "df[categorical_columns] = imputer_cat.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Add destination_key for unique ID (useful for SVD and tracking)\n",
    "df['destination_key'] = df['destination_city'] + '__' + df['destination_country']\n",
    "\n",
    "# Create combined textual features for embeddings and LLM prompts\n",
    "df['destination_description'] = (\n",
    "    df['destination_city'] + \" \" + df['destination_country'] + \" \" +\n",
    "    df['activities'].fillna('') + \" \" + df['climate'].fillna('')\n",
    ")\n",
    "df['user_profile_text'] = (\n",
    "    df['age'].astype(str) + \" \" + df['gender'] + \" \" + df['hobby'] + \" \" +\n",
    "    df['budget'] + \" \" + df['travel_style']\n",
    ")\n",
    "df['travel_rating'] = df['travel_rating'].astype(int) # Ensure rating is int for prompts\n",
    "\n",
    "print(f\"Original dataset loaded and preprocessed with {len(df)} samples.\")\n",
    "\n",
    "# Split the DataFrame into training and testing sets (80/20 ratio)\n",
    "print(\"Splitting dataset into 80/20 train-test sets...\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training dataset shape: {train_df.shape}\")\n",
    "print(f\"Testing dataset shape: {test_df.shape}\")\n",
    "print(\"Data splitting complete. Proceeding with model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2effc2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MiniLM Training: Generating Embeddings and Building Indices ---\n",
      "Loading SentenceTransformer (MiniLM) for embeddings...\n",
      "SentenceTransformer (MiniLM) loaded.\n",
      "Generating destination embeddings for training data...\n",
      "Generated 8000 training destination embeddings with dimension 384.\n",
      "\n",
      "--- Building FAISS Index for Deep Embeddings (from MiniLM) ---\n",
      "FAISS index built and saved to 'faiss_index_minilm.idx' with 8000 vectors.\n",
      "\n",
      "--- Building Annoy Index for Coarse Filtering (from MiniLM) ---\n",
      "Annoy index built and saved to 'annoy_index_minilm.ann'.\n",
      "\n",
      "MiniLM related training components (embeddings, FAISS, Annoy) are ready.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "# --- Ensure 'train_df' and 'device' are available from the previous cell ---\n",
    "\n",
    "print(\"\\n--- MiniLM Training: Generating Embeddings and Building Indices ---\")\n",
    "\n",
    "# 3. Generate Destination Embeddings (via MiniLM)\n",
    "# This model acts as your \"Fine-Tuned LLM\" for embedding generation in your flowchart.\n",
    "# Sentence-Transformers are already highly optimized for this.\n",
    "print(\"Loading SentenceTransformer (MiniLM) for embeddings...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_model.to(device) # Move embedding model to GPU if available\n",
    "print(\"SentenceTransformer (MiniLM) loaded.\")\n",
    "\n",
    "def generate_embedding_for_destination(text_series):\n",
    "    \"\"\"Generates sentence embeddings for a list of texts.\"\"\"\n",
    "    # Use the 'destination_description' created in the preprocessing step\n",
    "    return embedding_model.encode(text_series.tolist(), convert_to_numpy=True)\n",
    "\n",
    "print(\"Generating destination embeddings for training data...\")\n",
    "# Generate embeddings only for the training data\n",
    "train_embedding_matrix = generate_embedding_for_destination(train_df['destination_description'])\n",
    "train_embedding_matrix = train_embedding_matrix.astype('float32') # FAISS requires float32\n",
    "print(f\"Generated {len(train_embedding_matrix)} training destination embeddings with dimension {train_embedding_matrix.shape[1]}.\")\n",
    "\n",
    "# 4. Build Faiss Index for Deep Embeddings\n",
    "print(\"\\n--- Building FAISS Index for Deep Embeddings (from MiniLM) ---\")\n",
    "dimension = train_embedding_matrix.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension) # Using L2 distance for similarity search\n",
    "faiss_index.add(train_embedding_matrix)\n",
    "faiss.write_index(faiss_index, 'faiss_index_minilm.idx') # Save with a specific name\n",
    "print(f\"FAISS index built and saved to 'faiss_index_minilm.idx' with {faiss_index.ntotal} vectors.\")\n",
    "\n",
    "# 5. Build Annoy Index for Coarse Filtering\n",
    "print(\"\\n--- Building Annoy Index for Coarse Filtering (from MiniLM) ---\")\n",
    "annoy_index = AnnoyIndex(dimension, 'angular') # Using angular distance as common for embeddings\n",
    "for i in range(len(train_embedding_matrix)):\n",
    "    annoy_index.add_item(i, train_embedding_matrix[i])\n",
    "annoy_index.build(10) # 10 trees for indexing, balance between build time and search accuracy\n",
    "annoy_index.save('annoy_index_minilm.ann') # Save with a specific name\n",
    "print(\"Annoy index built and saved to 'annoy_index_minilm.ann'.\")\n",
    "\n",
    "print(\"\\nMiniLM related training components (embeddings, FAISS, Annoy) are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa2daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initiating Fine-Tuning for MiniLM Model ---\n",
      "Loaded 'all-MiniLM-L6-v2' for fine-tuning.\n",
      "Created evaluator for the test set.\n",
      "Starting fine-tuning. Model will be saved to './fine_tuned_minilm_travel'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Sts-test Pearson Cosine</th>\n",
       "      <th>Sts-test Spearman Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.043689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MiniLM Fine-Tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- MiniLM Fine-Tuning ---\n",
    "print(\"\\n--- Initiating Fine-Tuning for MiniLM Model ---\")\n",
    "\n",
    "# 1. Prepare the Data for Similarity Training\n",
    "# We need to create pairs of sentences: (user_profile, destination_description)\n",
    "# and a similarity score (the travel rating, normalized to be between 0 and 1).\n",
    "train_examples = []\n",
    "for _, row in train_df.iterrows():\n",
    "    user_profile = row['user_profile_text']\n",
    "    destination_desc = row['destination_description']\n",
    "    # Normalize the 1-5 rating to a 0.0-1.0 similarity score\n",
    "    score = (row['travel_rating'] - 1) / 4.0 \n",
    "    train_examples.append(InputExample(texts=[user_profile, destination_desc], label=score))\n",
    "\n",
    "# Create a DataLoader for the training set\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "# 2. Prepare the Evaluation Data\n",
    "eval_examples = []\n",
    "for _, row in test_df.iterrows():\n",
    "    user_profile = row['user_profile_text']\n",
    "    destination_desc = row['destination_description']\n",
    "    score = (row['travel_rating'] - 1) / 4.0\n",
    "    eval_examples.append(InputExample(texts=[user_profile, destination_desc], label=score))\n",
    "\n",
    "# 3. Load the Pre-Trained Model\n",
    "model_name_minilm = 'all-MiniLM-L6-v2'\n",
    "model_minilm = SentenceTransformer(model_name_minilm)\n",
    "model_minilm.to(device)\n",
    "print(f\"Loaded '{model_name_minilm}' for fine-tuning.\")\n",
    "\n",
    "# 4. Define the Loss Function\n",
    "# CosineSimilarityLoss is ideal for regression tasks where we want the cosine similarity\n",
    "# between two sentence embeddings to approximate a given score.\n",
    "train_loss = losses.CosineSimilarityLoss(model=model_minilm)\n",
    "\n",
    "# 5. Define the Evaluator\n",
    "# The evaluator will run on the test set after each epoch to measure performance.\n",
    "# It computes the Spearman rank correlation between the model's computed similarities and the true scores.\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(eval_examples, name='sts-test')\n",
    "print(\"Created evaluator for the test set.\")\n",
    "\n",
    "# 6. Fine-Tune the Model\n",
    "num_epochs = 1\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) # 10% of training steps for warmup\n",
    "output_path_minilm = \"./fine_tuned_minilm_travel\"\n",
    "\n",
    "print(f\"Starting fine-tuning. Model will be saved to '{output_path_minilm}'.\")\n",
    "\n",
    "model_minilm.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=500,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=output_path_minilm)\n",
    "\n",
    "print(\"\\n--- MiniLM Fine-Tuning Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3868dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BERT Training: Combined User-Destination Feature Learning ---\n",
      "Loading BERT tokenizer: bert-base-uncased...\n",
      "BERT tokenizer loaded.\n",
      "Creating prompt-based dataset for BERT fine-tuning...\n",
      "Tokenizing datasets for BERT...\n",
      "BERT dataset tokenization complete.\n",
      "Loading BERT model for regression: bert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model loaded.\n",
      "Starting BERT fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ausaf raza\\AppData\\Local\\Temp\\ipykernel_6460\\687497252.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_bert = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  49/1500 00:07 < 03:42, 6.52 it/s, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset as HFDataset # Alias to avoid conflict with surprise.Dataset\n",
    "\n",
    "# --- Ensure 'train_df', 'test_df' and 'device' are available from the previous cells ---\n",
    "import torch # Ensure torch is imported for torch.tensor usage\n",
    "\n",
    "print(\"\\n--- BERT Training: Combined User-Destination Feature Learning ---\")\n",
    "\n",
    "# Define model name for BERT\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "\n",
    "# Load BERT Tokenizer\n",
    "print(f\"Loading BERT tokenizer: {bert_model_name}...\")\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(bert_model_name)\n",
    "print(\"BERT tokenizer loaded.\")\n",
    "\n",
    "# Prepare dataset for BERT fine-tuning using a prompt-like structure\n",
    "def create_bert_prompt(row):\n",
    "    return (\n",
    "        f\"User is {int(row['age'])}, {row['gender']}, enjoys {row['hobby']}, \"\n",
    "        f\"budget is {row['budget']}, travel style is {row['travel_style']}. \"\n",
    "        f\"The destination is {row['destination_city']}, {row['destination_country']} offering {row['activities']} \"\n",
    "        f\"in a {row['climate']} climate.\"\n",
    "    )\n",
    "\n",
    "print(\"Creating prompt-based dataset for BERT fine-tuning...\")\n",
    "train_texts_bert = [create_bert_prompt(row) for _, row in train_df.iterrows()]\n",
    "train_labels_bert = train_df['travel_rating'].values.astype(float)\n",
    "\n",
    "test_texts_bert = [create_bert_prompt(row) for _, row in test_df.iterrows()]\n",
    "test_labels_bert = test_df['travel_rating'].values.astype(float)\n",
    "\n",
    "print(\"Tokenizing datasets for BERT...\")\n",
    "train_encodings_bert = tokenizer_bert(train_texts_bert, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "test_encodings_bert = tokenizer_bert(test_texts_bert, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "print(\"BERT dataset tokenization complete.\")\n",
    "\n",
    "# Custom Dataset class for BERT (for regression, so labels are floats)\n",
    "class BertTravelDataset(torch.utils.data.Dataset): # Inherit from torch.utils.data.Dataset directly\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "train_dataset_bert = BertTravelDataset(train_encodings_bert, train_labels_bert)\n",
    "test_dataset_bert = BertTravelDataset(test_encodings_bert, test_labels_bert)\n",
    "\n",
    "# Load BERT model for sequence classification (regression)\n",
    "print(f\"Loading BERT model for regression: {bert_model_name}...\")\n",
    "model_bert = BertForSequenceClassification.from_pretrained(bert_model_name, num_labels=1)\n",
    "model_bert.to(device)\n",
    "print(\"BERT model loaded.\")\n",
    "\n",
    "# Training arguments for BERT fine-tuning\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir='./fine_tuned_bert_travel',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_bert',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    # ADDED: This is the crucial line to prevent the AttributeError\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Trainer for BERT fine-tuning\n",
    "trainer_bert = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_dataset_bert,\n",
    "    eval_dataset=test_dataset_bert,\n",
    "    tokenizer=tokenizer_bert,\n",
    ")\n",
    "\n",
    "print(\"Starting BERT fine-tuning...\")\n",
    "trainer_bert.train()\n",
    "print(\"BERT fine-tuning complete.\")\n",
    "\n",
    "# Save the fine-tuned BERT model and tokenizer\n",
    "model_bert.save_pretrained(\"./fine_tuned_bert_travel\")\n",
    "tokenizer_bert.save_pretrained(\"./fine_tuned_bert_travel\")\n",
    "print(\"Fine-tuned BERT model and tokenizer saved to './fine_tuned_bert_travel'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d857a8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 'train_df' or 'device' not found. Ensure previous data loading/splitting cells are run.\n",
      "DEBUG: Re-initialized df and device for standalone cell execution. Device: cuda\n",
      "\n",
      "--- DeepSeek Training: Generative Fine-tuning with Prompts ---\n",
      "Loading LLM tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "LLM tokenizer loaded.\n",
      "Loading LLM model with quantization and explicit device mapping...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading LLM model with quantization and explicit device mapping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Enable gradient checkpointing & prepare for QLoRA training\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Gradient checkpointing reduces VRAM usage by recomputing activations, but slows down training\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[43mmodel_llm\u001b[49m\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[0;32m     66\u001b[0m model_llm \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(model_llm)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Apply LoRA configuration (Parameter Efficient Fine-Tuning)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_llm' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset as HFDataset # Alias to avoid conflict if you also use other 'Dataset' classes\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch # Ensure torch is imported\n",
    "\n",
    "# --- Ensure 'train_df' and 'device' are available from previous cells ---\n",
    "# If running this cell in isolation, you might need to re-initialize 'device' and load 'train_df'.\n",
    "try:\n",
    "    train_df # Check if train_df exists\n",
    "    device # Check if device exists\n",
    "except NameError:\n",
    "    print(\"WARNING: 'train_df' or 'device' not found. Ensure previous data loading/splitting cells are run.\")\n",
    "    # Fallback/re-initialization for demonstration if run separately (remove for actual sequential notebook use)\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    # Minimal reload for df properties used in prompts\n",
    "    df = pd.read_csv('travel_recommendations_10k.csv')\n",
    "    numerical_columns = ['age', 'travel_rating']\n",
    "    categorical_columns = ['gender', 'hobby', 'budget', 'travel_style', 'activities', 'climate']\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    df[numerical_columns] = imputer_num.fit_transform(df[numerical_columns])\n",
    "    df[categorical_columns] = imputer_cat.fit_transform(df[categorical_columns])\n",
    "    df['destination_key'] = df['destination_city'] + '__' + df['destination_country']\n",
    "    df['destination_description'] = (\n",
    "        df['destination_city'] + \" \" + df['destination_country'] + \" \" +\n",
    "        df['activities'].fillna('') + \" \" + df['climate'].fillna('')\n",
    "    )\n",
    "    df['user_profile_text'] = (\n",
    "        df['age'].astype(str) + \" \" + df['gender'] + \" \" + df['hobby'] + \" \" +\n",
    "        df['budget'] + \" \" + df['travel_style']\n",
    "    )\n",
    "    df['travel_rating'] = df['travel_rating'].astype(int)\n",
    "    train_df, _ = train_test_split(df, test_size=0.2, random_state=42) # Only need train_df for this cell\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"DEBUG: Re-initialized df and device for standalone cell execution. Device: {device}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- DeepSeek Training: Generative Fine-tuning with Prompts ---\")\n",
    "\n",
    "# Define model name\n",
    "model_name_llm = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# Load tokenizer for LLM\n",
    "print(f\"Loading LLM tokenizer: {model_name_llm}...\")\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(model_name_llm, trust_remote_code=True)\n",
    "if tokenizer_llm.pad_token is None:\n",
    "    tokenizer_llm.pad_token = tokenizer_llm.eos_token # Essential for batch processing\n",
    "print(\"LLM tokenizer loaded.\")\n",
    "\n",
    "# Quantization config for memory efficiency (crucial for 4GB VRAM)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True # Enables offloading to CPU RAM if VRAM is insufficient\n",
    ")\n",
    "\n",
    "# Load LLM model with quantization and explicit device mapping\n",
    "print(\"Loading LLM model with quantization and explicit device mapping...\")\n",
    "  \n",
    "\n",
    "\n",
    "# Enable gradient checkpointing & prepare for QLoRA training\n",
    "# Gradient checkpointing reduces VRAM usage by recomputing activations, but slows down training\n",
    "model_llm.gradient_checkpointing_enable()\n",
    "model_llm = prepare_model_for_kbit_training(model_llm)\n",
    "\n",
    "# Apply LoRA configuration (Parameter Efficient Fine-Tuning)\n",
    "print(\"Applying LoRA to LLM...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank of the update matrices. Lower 'r' means fewer trainable parameters.\n",
    "    lora_alpha=32, # Scaling factor for LoRA updates.\n",
    "    lora_dropout=0.1, # Dropout probability for LoRA layers.\n",
    "    bias=\"none\", # Don't apply LoRA to bias terms.\n",
    "    task_type=\"CAUSAL_LM\" # Specify the task type for the model.\n",
    ")\n",
    "model_llm = get_peft_model(model_llm, lora_config)\n",
    "print(\"LoRA applied. Trainable parameters:\")\n",
    "model_llm.print_trainable_parameters() # This will show how few parameters are actually being trained\n",
    "\n",
    "# --- Prompt Engineering for Fine-Tuning ---\n",
    "# This function creates a structured prompt string for each row of your training dataset.\n",
    "# The LLM will learn to predict the 'Answer:' based on the 'Question:' and 'Context:'.\n",
    "def create_fine_tuning_prompt_deepseek(row):\n",
    "    # This structure trains the model to associate a user's profile with a destination's rating.\n",
    "    # It explicitly defines the context and the desired output format.\n",
    "    prompt = (\n",
    "        f\"User Profile: Age {int(row['age'])}, {row['gender']}, enjoys {row['hobby']}, \"\n",
    "        f\"budget is {row['budget']}, travel style is {row['travel_style']}.\\n\"\n",
    "        f\"Destination Details: City {row['destination_city']}, Country {row['destination_country']}, \"\n",
    "        f\"activities include {row['activities']}, climate is {row['climate']}.\\n\"\n",
    "        f\"Question: What is the travel rating for this destination by this user? Answer: {int(row['travel_rating'])}\"\n",
    "    )\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "print(\"Creating prompt-based dataset for DeepSeek fine-tuning...\")\n",
    "# Use only the training data (train_df) for fine-tuning the LLM\n",
    "prompts_llm_data = [create_fine_tuning_prompt_deepseek(row) for index, row in train_df.iterrows()]\n",
    "dataset_llm = HFDataset.from_list(prompts_llm_data)\n",
    "print(f\"Created {len(dataset_llm)} prompts.\")\n",
    "print(f\"Example prompt for DeepSeek:\\n{dataset_llm[0]['text']}\") # Display an example prompt\n",
    "\n",
    "def tokenize_fn_llm(examples):\n",
    "    # This will generate input_ids and attention_mask.\n",
    "    # DataCollatorForLanguageModeling will create 'labels' internally for causal LM.\n",
    "    return tokenizer_llm(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "print(\"Tokenizing dataset for DeepSeek fine-tuning...\")\n",
    "tokenized_dataset_llm = dataset_llm.map(tokenize_fn_llm, batched=True)\n",
    "print(\"DeepSeek dataset tokenization complete.\")\n",
    "\n",
    "# Training arguments for LLM fine-tuning\n",
    "training_args_llm = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_deepseek_travel_prompts\", # Output directory for saved model checkpoints\n",
    "    per_device_train_batch_size=1, # Keep 1 for memory, use gradient_accumulation for effective batch size\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients over 4 batches, effective batch size = 1 * 4 = 4\n",
    "    num_train_epochs=3, # Number of full passes over the training data\n",
    "    logging_steps=100, # Log training progress every 100 steps\n",
    "    save_steps=500, # Save model checkpoint every 500 steps\n",
    "    save_total_limit=2, # Keep only the 2 most recent checkpoints\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision (FP16) if GPU is available (saves VRAM)\n",
    "    report_to=\"none\", # Disable reporting to external services like Weights & Biases\n",
    "    eval_strategy=\"no\",\n",
    "    # remove_unused_columns=False, # Often helpful with custom datasets, though DataCollator usually handles this.\n",
    "                               # Uncomment if you face similar AttributeError as with BERT.\n",
    ")\n",
    "\n",
    "# Initialize DataCollator for Language Modeling\n",
    "# mlm=False for Causal Language Modeling (decoder-only LLMs)\n",
    "data_collator_llm = DataCollatorForLanguageModeling(tokenizer=tokenizer_llm, mlm=False)\n",
    "\n",
    "# Trainer for LLM fine-tuning\n",
    "trainer_llm = Trainer(\n",
    "    model=model_llm,\n",
    "    args=training_args_llm,\n",
    "    train_dataset=tokenized_dataset_llm,\n",
    "    eval_dataset=tokenized_dataset_eva,\n",
    "    tokenizer=tokenizer_llm, # Provide tokenizer to Trainer for internal use (e.g., data collator)\n",
    "    data_collator=data_collator_llm, # Pass the data collator to handle input formatting\n",
    ")\n",
    "\n",
    "print(\"Starting DeepSeek fine-tuning...\")\n",
    "trainer_llm.train()\n",
    "print(\"DeepSeek fine-tuning complete.\")\n",
    "\n",
    "# Save the final fine-tuned LLM and its tokenizer\n",
    "model_llm.save_pretrained(\"./fine_tuned_deepseek_travel_prompts\")\n",
    "tokenizer_llm.save_pretrained(\"./fine_tuned_deepseek_travel_prompts\")\n",
    "print(\"Fine-tuned DeepSeek model and tokenizer saved to './fine_tuned_deepseek_travel_prompts'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9b9e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Enabled torch.autograd.set_detect_anomaly(True) for debugging.\n",
      "\n",
      "--- Initial GPU Check ---\n",
      "torch.cuda.is_available(): True\n",
      "Current CUDA device: 0\n",
      "CUDA Device Name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA Device Capability: (8, 6)\n",
      "CUDA Device Total Memory: 3.23 GB\n",
      "CUDA Device Free Memory: 4.00 GB\n",
      "CUDA Device Allocated Memory: 0.00 GB\n",
      "CUDA Device Reserved Memory: 0.00 GB\n",
      "-------------------------\n",
      "\n",
      "--- Starting Dynamic LLM & SVD Retraining Process ---\n",
      "Please ensure no other processes are locking the model save directories (e.g., fine_tuned_bert_travel, fine_tuned_deepseek_travel_prompts).\n",
      "Consider deleting these directories before running if you encounter persistent file errors.\n",
      "Loading historical data for retraining...\n",
      "No new feedback CSV provided or found.\n",
      "\n",
      "--- Initiating DeepSeek Fine-tuning ---\n",
      "\n",
      "--- DeepSeek Incremental Fine-tuning ---\n",
      "DeepSeek base model loaded onto device: cuda:0\n",
      "Loading existing LoRA weights from ./fine_tuned_deepseek_travel_prompts for incremental training.\n",
      "DeepSeek LoRA model loaded onto device: cuda:0\n",
      "LoRA applied. Trainable parameters:\n",
      "trainable params: 0 || all params: 1,778,177,536 || trainable%: 0.0000\n",
      "\n",
      "--- Diagnostic: Checking for None gradients in trainable parameters ---\n",
      "All trainable parameters have non-None gradients (or don't require gradients) at this stage.\n",
      "---------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8000/8000 [00:01<00:00, 5239.56 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 6144.36 examples/s]\n",
      "C:\\Users\\Ausaf raza\\AppData\\Local\\Temp\\ipykernel_17256\\1666722800.py:275: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_llm = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared CUDA cache before DeepSeek training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  18/2000 10:43 < 22:08:44, 0.02 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 349\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# --- REORDERING: DeepSeek first, then BERT, then SVD ---\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Initiating DeepSeek Fine-tuning ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 349\u001b[0m deepseek_model_retrained, deepseek_tokenizer_retrained \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_deepseek_model_incremental\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Initiating BERT Fine-tuning ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    352\u001b[0m bert_model_retrained, bert_tokenizer_retrained \u001b[38;5;241m=\u001b[39m fine_tune_bert_model_incremental(train_df_updated, test_df_updated, device)\n",
      "Cell \u001b[1;32mIn[1], line 288\u001b[0m, in \u001b[0;36mfine_tune_deepseek_model_incremental\u001b[1;34m(train_df, test_df, device)\u001b[0m\n\u001b[0;32m    285\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleared CUDA cache before DeepSeek training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 288\u001b[0m \u001b[43mtrainer_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m max_retries \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n",
      "File \u001b[1;32mc:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\transformers\\trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\transformers\\trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2553\u001b[0m )\n\u001b[0;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2561\u001b[0m ):\n\u001b[0;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\transformers\\trainer.py:3791\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3789\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\accelerate\\accelerator.py:2473\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2473\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time # Import for time.sleep\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments,\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.trainer_utils import EvaluationStrategy, SaveStrategy\n",
    "from transformers.trainer_utils import IntervalStrategy as LoggingStrategy \n",
    "\n",
    "from safetensors import SafetensorError\n",
    "\n",
    "from datasets import Dataset as HFDataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from surprise import Reader, Dataset, SVD, dump \n",
    "import random\n",
    "import os\n",
    "\n",
    "# --- Utility Functions (from your project) ---\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class BertTravelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "def create_bert_prompt(row):\n",
    "    return (\n",
    "        f\"User is {int(row['age'])}, {row['gender']}, enjoys {row['hobby']}, \"\n",
    "        f\"budget is {row['budget']}, travel style is {row['travel_style']}. \"\n",
    "        f\"The destination is {row['destination_city']}, {row['destination_country']} offering {row['activities']} \"\n",
    "        f\"in a {row['climate']} climate.\"\n",
    "    )\n",
    "\n",
    "def create_fine_tuning_prompt_deepseek(row):\n",
    "    prompt = (\n",
    "        f\"User Profile: Age {int(row['age'])}, {row['gender']}, enjoys {row['hobby']}, \"\n",
    "        f\"budget is {row['budget']}, travel style is {row['travel_style']}.\\n\"\n",
    "        f\"Destination Details: City {row['destination_city']}, Country {row['destination_country']}, \"\n",
    "        f\"activities include {row['activities']}, climate is {row['climate']}.\\n\"\n",
    "        f\"Question: What is the travel rating for this destination by this user? Answer: {int(row['travel_rating'])}\"\n",
    "    )\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# --- Function to load and preprocess data for retraining ---\n",
    "def load_and_preprocess_data_for_retraining(csv_path='travel_recommendations_10k.csv', new_feedback_csv_path=None):\n",
    "    print(\"Loading historical data for retraining...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    numerical_columns = ['age', 'travel_rating']\n",
    "    categorical_columns = ['gender', 'hobby', 'budget', 'travel_style', 'activities', 'climate']\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    df[numerical_columns] = imputer_num.fit_transform(df[numerical_columns])\n",
    "    df[categorical_columns] = imputer_cat.fit_transform(df[categorical_columns])\n",
    "    df['destination_key'] = df['destination_city'] + '__' + df['destination_country']\n",
    "    df['destination_description'] = (\n",
    "        df['destination_city'] + \" \" + df['destination_country'] + \" \" +\n",
    "        df['activities'].fillna('') + \" \" + df['climate'].fillna('')\n",
    "    )\n",
    "    df['user_profile_text'] = (\n",
    "        df['age'].astype(str) + \" \" + df['gender'] + \" \" + df['hobby'] + \" \" +\n",
    "        df['budget'] + \" \" + df['travel_style']\n",
    "    )\n",
    "    df['travel_rating'] = df['travel_rating'].astype(int)\n",
    "\n",
    "    if new_feedback_csv_path and os.path.exists(new_feedback_csv_path):\n",
    "        print(f\"Loading new feedback from {new_feedback_csv_path}...\")\n",
    "        new_df = pd.read_csv(new_feedback_csv_path)\n",
    "        new_df[numerical_columns] = imputer_num.transform(new_df[numerical_columns])\n",
    "        new_df[categorical_columns] = imputer_cat.transform(new_df[categorical_columns])\n",
    "        new_df['destination_key'] = new_df['destination_city'] + '__' + new_df['destination_country']\n",
    "        new_df['destination_description'] = (\n",
    "            new_df['destination_city'] + \" \" + new_df['destination_country'] + \" \" +\n",
    "            new_df['activities'].fillna('') + \" \" + new_df['climate'].fillna('')\n",
    "        )\n",
    "        new_df['user_profile_text'] = (\n",
    "            new_df['age'].astype(str) + \" \" + new_df['gender'] + \" \" + new_df['hobby'] + \" \" +\n",
    "            new_df['budget'] + \" \" + new_df['travel_style']\n",
    "        )\n",
    "        new_df['travel_rating'] = new_df['travel_rating'].astype(int)\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "        print(f\"Combined data now has {len(df)} samples.\")\n",
    "    else:\n",
    "        print(\"No new feedback CSV provided or found.\")\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    return train_df, test_df\n",
    "\n",
    "# --- Function to fine-tune BERT incrementally ---\n",
    "def fine_tune_bert_model_incremental(train_df, test_df, device):\n",
    "    print(\"\\n--- BERT Incremental Fine-tuning ---\")\n",
    "    bert_model_name = 'bert-base-uncased'\n",
    "    model_path = \"./fine_tuned_bert_travel\"\n",
    "    \n",
    "    tokenizer_bert = BertTokenizer.from_pretrained(model_path if os.path.exists(model_path) else bert_model_name)\n",
    "    model_bert = BertForSequenceClassification.from_pretrained(model_path if os.path.exists(model_path) else bert_model_name, num_labels=1)\n",
    "    model_bert.to(device)\n",
    "    print(f\"BERT model loaded onto device: {model_bert.device}\")\n",
    "\n",
    "    train_texts_bert = [create_bert_prompt(row) for _, row in train_df.iterrows()]\n",
    "    train_labels_bert = train_df['travel_rating'].values.astype(float)\n",
    "    test_texts_bert = [create_bert_prompt(row) for _, row in test_df.iterrows()]\n",
    "    test_labels_bert = test_df['travel_rating'].values.astype(float)\n",
    "\n",
    "    train_encodings_bert = tokenizer_bert(train_texts_bert, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    test_encodings_bert = tokenizer_bert(test_texts_bert, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "    train_dataset_bert = BertTravelDataset(train_encodings_bert, train_labels_bert)\n",
    "    test_dataset_bert = BertTravelDataset(test_encodings_bert, test_labels_bert)\n",
    "\n",
    "    training_args_bert = TrainingArguments(\n",
    "        output_dir=model_path,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=50,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs_bert_retrain',\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    try:\n",
    "        training_args_bert.evaluation_strategy = EvaluationStrategy.EPOCH\n",
    "        training_args_bert.save_strategy = SaveStrategy.EPOCH\n",
    "        training_args_bert.logging_strategy = LoggingStrategy.STEPS\n",
    "    except AttributeError:\n",
    "        print(\"CRITICAL WARNING: Could not set evaluation/save/logging strategies as attributes. Your transformers version is likely severely outdated/corrupted or has a non-standard API.\")\n",
    "        print(\"Training progress and saving behavior might be unpredictable.\")\n",
    "\n",
    "    trainer_bert = Trainer(\n",
    "        model=model_bert,\n",
    "        args=training_args_bert,\n",
    "        train_dataset=train_dataset_bert,\n",
    "        eval_dataset=test_dataset_bert, \n",
    "        tokenizer=tokenizer_bert,\n",
    "    )\n",
    "    trainer_bert.train()\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model_bert.save_pretrained(model_path)\n",
    "            tokenizer_bert.save_pretrained(model_path)\n",
    "            print(f\"BERT incremental fine-tuning complete. Model saved to {model_path}\")\n",
    "            break \n",
    "        except SafetensorError as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} to save BERT model failed with SafetensorError: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"Max retries reached. BERT model save failed.\")\n",
    "    return model_bert, tokenizer_bert\n",
    "\n",
    "# --- Function to fine-tune DeepSeek incrementally ---\n",
    "def fine_tune_deepseek_model_incremental(train_df, test_df, device): \n",
    "    print(\"\\n--- DeepSeek Incremental Fine-tuning ---\")\n",
    "    model_name_llm_base = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    model_path = \"./fine_tuned_deepseek_travel_prompts\"\n",
    "\n",
    "    tokenizer_llm = AutoTokenizer.from_pretrained(model_path if os.path.exists(model_path) else model_name_llm_base, trust_remote_code=True)\n",
    "    if tokenizer_llm.pad_token is None:\n",
    "        tokenizer_llm.pad_token = tokenizer_llm.eos_token\n",
    "\n",
    "    # Keep BitsAndBytesConfig for 8-bit loading\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, \n",
    "        # llm_int8_enable_fp32_cpu_offload=True # Can comment out if not strictly needed for memory\n",
    "    )\n",
    "    \n",
    "    # Set torch_dtype for the base model loading.\n",
    "    # If not using fp16/bf16 training (as per changes below), default to float32 or bfloat16 if GPU supports.\n",
    "    # Given the previous error, using float32 for training clarity might be better.\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_llm_base,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={'': torch.cuda.current_device()} if torch.cuda.is_available() else \"cpu\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float32 # Changed to float32 for full precision operations if AMP is off\n",
    "    )\n",
    "    print(f\"DeepSeek base model loaded onto device: {base_model.device}\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing LoRA weights from {model_path} for incremental training.\")\n",
    "        model_llm = PeftModel.from_pretrained(base_model, model_path).to(device)\n",
    "    else:\n",
    "        print(f\"No existing LoRA weights found. Applying LoRA for the first time.\")\n",
    "        model_llm = prepare_model_for_kbit_training(base_model)\n",
    "        lora_config = LoraConfig(r=8, lora_alpha=32, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "        model_llm = get_peft_model(model_llm, lora_config)\n",
    "        model_llm.to(device) \n",
    "    print(f\"DeepSeek LoRA model loaded onto device: {model_llm.device}\")\n",
    "\n",
    "    model_llm.enable_input_require_grads() \n",
    "    model_llm.gradient_checkpointing_enable()\n",
    "    print(\"LoRA applied. Trainable parameters:\")\n",
    "    model_llm.print_trainable_parameters()\n",
    "\n",
    "    # --- Diagnostic: Check for None gradients (before Trainer starts) ---\n",
    "    print(\"\\n--- Diagnostic: Checking for None gradients in trainable parameters ---\")\n",
    "    none_grad_params = []\n",
    "    for name, param in model_llm.named_parameters():\n",
    "        # param.grad might be None if no backward pass has occurred yet.\n",
    "        # This check is more indicative if done inside a training loop, but as an initial check:\n",
    "        if param.requires_grad and param.grad is None:\n",
    "            none_grad_params.append(name)\n",
    "    if none_grad_params:\n",
    "        print(f\"WARNING: The following trainable parameters have None gradients BEFORE training starts: {none_grad_params}\")\n",
    "        print(\"This is normal at initialization. If this persists during training after a backward pass, it indicates an issue.\")\n",
    "    else:\n",
    "        print(\"All trainable parameters have non-None gradients (or don't require gradients) at this stage.\")\n",
    "    print(\"---------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    prompts_llm_data = [create_fine_tuning_prompt_deepseek(row) for index, row in train_df.iterrows()]\n",
    "    tokenized_train_dataset_llm = HFDataset.from_list(prompts_llm_data).map(lambda examples: tokenizer_llm(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512), batched=True)\n",
    "\n",
    "    prompts_llm_eval_data = [create_fine_tuning_prompt_deepseek(row) for index, row in test_df.iterrows()]\n",
    "    tokenized_eval_dataset_llm = HFDataset.from_list(prompts_llm_eval_data).map(lambda examples: tokenizer_llm(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512), batched=True)\n",
    "\n",
    "    training_args_llm = TrainingArguments(\n",
    "        output_dir=model_path,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        \n",
    "        # --- CRITICAL CHANGE: Choose a different 8-bit optimizer and remove AMP flags ---\n",
    "        # \"paged_adamw_8bit\" is often more stable with 8-bit quantization and PEFT\n",
    "        optim=\"paged_adamw_8bit\", \n",
    "        \n",
    "        # Explicitly remove fp16=False and bf16=False from here.\n",
    "        # If we are using optim=\"paged_adamw_8bit\", the scaler should ideally not be involved.\n",
    "        # If it's still involved and causing issues, then accelerate's internal logic needs investigation.\n",
    "        # By removing these explicit 'False' settings, we let the optimizer handle its precision.\n",
    "        \n",
    "        learning_rate=5e-6, # Keep reduced learning rate for stability\n",
    "        max_grad_norm=1.0,  # Keep gradient clipping for stability\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        training_args_llm.evaluation_strategy = EvaluationStrategy.EPOCH\n",
    "        training_args_llm.save_strategy = SaveStrategy.EPOCH\n",
    "        training_args_llm.logging_strategy = LoggingStrategy.STEPS\n",
    "    except AttributeError:\n",
    "        print(\"CRITICAL WARNING: Could not set evaluation/save/logging strategies as attributes for DeepSeek. Your transformers version is likely severely outdated/corrupted or has a non-standard API.\")\n",
    "        print(\"Training progress and saving behavior might be unpredictable.\")\n",
    "\n",
    "    data_collator_llm = DataCollatorForLanguageModeling(tokenizer=tokenizer_llm, mlm=False)\n",
    "    trainer_llm = Trainer(\n",
    "        model=model_llm,\n",
    "        args=training_args_llm,\n",
    "        train_dataset=tokenized_train_dataset_llm,\n",
    "        eval_dataset=tokenized_eval_dataset_llm,  \n",
    "        tokenizer=tokenizer_llm,\n",
    "        data_collator=data_collator_llm,\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared CUDA cache before DeepSeek training.\")\n",
    "\n",
    "    trainer_llm.train()\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model_llm.save_pretrained(model_path)\n",
    "            tokenizer_llm.save_pretrained(model_path)\n",
    "            print(f\"DeepSeek incremental fine-tuning complete. Model saved to {model_path}\")\n",
    "            break\n",
    "        except SafetensorError as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} to save DeepSeek model failed with SafetensorError: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"Max retries reached. DeepSeek model save failed.\")\n",
    "    return model_llm, tokenizer_llm\n",
    "\n",
    "# --- Main execution block for dynamic retraining ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure this is at the very top for effective debugging.\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    print(\"Enabled torch.autograd.set_detect_anomaly(True) for debugging.\")\n",
    "\n",
    "    set_seed()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n--- Initial GPU Check ---\")\n",
    "    print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"CUDA Device Name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "        print(f\"CUDA Device Capability: {torch.cuda.get_device_capability(torch.cuda.current_device())}\")\n",
    "        try:\n",
    "            total_mem, free_mem = torch.cuda.mem_get_info()\n",
    "            allocated_mem = torch.cuda.memory_allocated()\n",
    "            reserved_mem = torch.cuda.memory_reserved()\n",
    "            print(f\"CUDA Device Total Memory: {total_mem / (1024**3):.2f} GB\")\n",
    "            print(f\"CUDA Device Free Memory: {free_mem / (1024**3):.2f} GB\")\n",
    "            print(f\"CUDA Device Allocated Memory: {allocated_mem / (1024**3):.2f} GB\")\n",
    "            print(f\"CUDA Device Reserved Memory: {reserved_mem / (1024**3):.2f} GB\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Could not get GPU memory info: {e}. This might happen if no device is selected or memory query fails.\")\n",
    "    else:\n",
    "        print(\"CUDA is NOT available in PyTorch. Training will be on CPU, which will be slow.\")\n",
    "        print(\"Please ensure NVIDIA drivers, CUDA Toolkit, and PyTorch CUDA version are compatible.\")\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    # --- Simulate new user feedback by creating a dummy CSV ---\n",
    "    NEW_FEEDBACK_CSV = 'new_user_feedback.csv' \n",
    "\n",
    "    print(\"\\n--- Starting Dynamic LLM & SVD Retraining Process ---\")\n",
    "    print(\"Please ensure no other processes are locking the model save directories (e.g., fine_tuned_bert_travel, fine_tuned_deepseek_travel_prompts).\")\n",
    "    print(\"Consider deleting these directories before running if you encounter persistent file errors.\")\n",
    "    \n",
    "    train_df_updated, test_df_updated = load_and_preprocess_data_for_retraining(\n",
    "        csv_path='travel_recommendations_10k.csv',\n",
    "        new_feedback_csv_path=NEW_FEEDBACK_CSV\n",
    "    )\n",
    "\n",
    "    # --- REORDERING: DeepSeek first, then BERT, then SVD ---\n",
    "    print(\"\\n--- Initiating DeepSeek Fine-tuning ---\")\n",
    "    deepseek_model_retrained, deepseek_tokenizer_retrained = fine_tune_deepseek_model_incremental(train_df_updated, test_df_updated, device) \n",
    "\n",
    "    print(\"\\n--- Initiating BERT Fine-tuning ---\")\n",
    "    bert_model_retrained, bert_tokenizer_retrained = fine_tune_bert_model_incremental(train_df_updated, test_df_updated, device)\n",
    "\n",
    "    print(\"\\n--- Retraining SVD Model with updated data ---\")\n",
    "    cf_train_df = train_df_updated[['user_id', 'destination_key', 'travel_rating']].copy()\n",
    "    cf_train_df['user_id'] = cf_train_df['user_id'].astype(str)\n",
    "    cf_train_df['travel_rating'] = cf_train_df['travel_rating'].astype(float)\n",
    "\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    svd_data = Dataset.load_from_df(cf_train_df[['user_id', 'destination_key', 'travel_rating']], reader)\n",
    "    svd_trainset = svd_data.build_full_trainset()\n",
    "\n",
    "    svd_model = SVD()\n",
    "    svd_model.fit(svd_trainset)\n",
    "\n",
    "    dump.dump('./svd_model.pkl', algo=svd_model)\n",
    "\n",
    "    print(\"--- All models retrained and saved successfully ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7aaa149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should print your GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c0df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base DeepSeek model...\n",
      "Loading LoRA adapter from: ./fine_tuned_deepseek_travel_prompts\n",
      "Fine-tuned model loaded successfully.\n",
      "\n",
      "Generating new destination embeddings using the fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Destinations:   0%|          | 0/8000 [00:00<?, ?it/s]c:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Processing Destinations: 100%|██████████| 8000/8000 [36:06<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding generation complete.\n",
      "Shape of the new embedding matrix: (8000, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "# --- 1. Setup and Model Loading ---\n",
    "\n",
    "# Ensure the device is set correctly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths for your models\n",
    "base_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "adapter_path = \"./fine_tuned_deepseek_travel_prompts\" # Path to your fine-tuned LoRA adapter\n",
    "\n",
    "# Check if the adapter path exists\n",
    "if not os.path.exists(adapter_path):\n",
    "    raise FileNotFoundError(f\"Adapter directory not found at {adapter_path}. Please ensure fine-tuning was successful.\")\n",
    "\n",
    "# Load the tokenizer from your fine-tuned model directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Configure BitsAndBytes for 8-bit loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# Load the base model with quantization\n",
    "print(\"Loading base DeepSeek model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": device}, # Automatically map to the current device\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Load the PEFT model by applying the adapter to the base model\n",
    "print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model.eval() # Set the model to evaluation mode\n",
    "print(\"Fine-tuned model loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- 2. Define the Embedding Function ---\n",
    "\n",
    "def get_embedding_from_finetuned_llm(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates an embedding for a given text using the fine-tuned causal LLM.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text and move it to the GPU\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for faster inference\n",
    "        # Get model outputs, including the hidden states\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Get the last hidden state from the outputs\n",
    "    last_hidden_state = outputs.hidden_states[-1] # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "    # Perform mean pooling: average the hidden states across the sequence length dimension\n",
    "    # This creates a single vector representing the entire text\n",
    "    pooled_embedding = last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    # Move the final embedding to the CPU and convert to a NumPy array\n",
    "    return pooled_embedding.cpu().numpy()\n",
    "\n",
    "\n",
    "# --- 3. Generate Embeddings for All Destinations ---\n",
    "\n",
    "# Assuming 'train_df' is loaded from your previous data preparation steps\n",
    "# If not, you need to load and preprocess your data first.\n",
    "# For example: train_df, _ = load_and_preprocess_data_for_retraining()\n",
    "\n",
    "print(\"\\nGenerating new destination embeddings using the fine-tuned model...\")\n",
    "\n",
    "# Create a list to store the new embeddings\n",
    "new_destination_embeddings = []\n",
    "\n",
    "# Loop through each destination description in your training data\n",
    "for description in tqdm(train_df['destination_description'], desc=\"Processing Destinations\"):\n",
    "    embedding = get_embedding_from_finetuned_llm(description, model, tokenizer)\n",
    "    new_destination_embeddings.append(embedding)\n",
    "\n",
    "# Convert the list of embeddings into a single NumPy matrix\n",
    "# FAISS requires the matrix to be of type float32\n",
    "embedding_matrix_deepseek = np.array(new_destination_embeddings).astype('float32')\n",
    "\n",
    "print(f\"\\nEmbedding generation complete.\")\n",
    "print(f\"Shape of the new embedding matrix: {embedding_matrix_deepseek.shape}\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "del model\n",
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331d4692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building FAISS Index with DeepSeek Embeddings ---\n",
      "FAISS index built and saved to 'faiss_index_deepseek.idx' with 8000 vectors.\n",
      "\n",
      "--- Building Annoy Index with DeepSeek Embeddings ---\n",
      "Annoy index built and saved to 'annoy_index_deepseek.ann'.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "# --- 1. Build and Save the New Faiss Index ---\n",
    "\n",
    "print(\"\\n--- Building FAISS Index with DeepSeek Embeddings ---\")\n",
    "\n",
    "# The embedding dimension is taken from the new matrix\n",
    "dimension = embedding_matrix_deepseek.shape[1]\n",
    "new_faiss_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add the new embeddings to the index\n",
    "new_faiss_index.add(embedding_matrix_deepseek)\n",
    "\n",
    "# Save the new index with a new name\n",
    "faiss_index_path = 'faiss_index_deepseek.idx'\n",
    "faiss.write_index(new_faiss_index, faiss_index_path)\n",
    "\n",
    "print(f\"FAISS index built and saved to '{faiss_index_path}' with {new_faiss_index.ntotal} vectors.\")\n",
    "\n",
    "\n",
    "# --- 2. Build and Save the New Annoy Index ---\n",
    "\n",
    "print(\"\\n--- Building Annoy Index with DeepSeek Embeddings ---\")\n",
    "\n",
    "# Initialize the Annoy index\n",
    "new_annoy_index = AnnoyIndex(dimension, 'angular') # Using angular distance\n",
    "\n",
    "# Add items to the Annoy index\n",
    "for i in range(len(embedding_matrix_deepseek)):\n",
    "    new_annoy_index.add_item(i, embedding_matrix_deepseek[i])\n",
    "\n",
    "# Build the index (10 trees is a good starting point)\n",
    "new_annoy_index.build(10)\n",
    "\n",
    "# Save the new index with a new name\n",
    "annoy_index_path = 'annoy_index_deepseek.ann'\n",
    "new_annoy_index.save(annoy_index_path)\n",
    "\n",
    "print(f\"Annoy index built and saved to '{annoy_index_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c53a0",
   "metadata": {},
   "source": [
    "now we move on to inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea855e2",
   "metadata": {},
   "source": [
    "Part 1: Setup - Loading All Models and Indexes\n",
    "First, we need to load everything we prepared in the training phase: the fine-tuned model, the tokenizer, the Faiss and Annoy indexes, and the dataframe that holds our destination information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f74b9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading tokenizer...\n",
      "Loading fine-tuned model...\n",
      "Model loaded successfully.\n",
      "Loading FAISS index...\n",
      "Loading Annoy index...\n",
      "Indexes loaded successfully.\n",
      "Loading destination data...\n",
      "Destination data loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import re # For parsing the model's output\n",
    "\n",
    "# --- Configuration and Paths ---\n",
    "BASE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "ADAPTER_PATH = \"./fine_tuned_deepseek_travel_prompts\"\n",
    "FAISS_PATH = \"./faiss_index_deepseek.idx\"\n",
    "ANNOY_PATH = \"./annoy_index_deepseek.ann\"\n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "EMBEDDING_DIMENSION = 1536 # From your previous output\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Fine-Tuned LLM and Tokenizer ---\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading fine-tuned model...\")\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": device},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval() # Set to evaluation mode\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# --- Load FAISS and Annoy Indexes ---\n",
    "print(\"Loading FAISS index...\")\n",
    "faiss_index = faiss.read_index(FAISS_PATH)\n",
    "\n",
    "print(\"Loading Annoy index...\")\n",
    "annoy_index = AnnoyIndex(EMBEDDING_DIMENSION, 'angular')\n",
    "annoy_index.load(ANNOY_PATH)\n",
    "print(\"Indexes loaded successfully.\")\n",
    "\n",
    "# --- Load and Preprocess Data ---\n",
    "# We need the original dataframe to map the search results (which are integer IDs)\n",
    "# back to actual destination names and details.\n",
    "print(\"Loading destination data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "# This should be the same preprocessing as in your training script\n",
    "df['destination_description'] = (\n",
    "    df['destination_city'] + \" \" + df['destination_country'] + \" \" +\n",
    "    df['activities'].fillna('') + \" \" + df['climate'].fillna('')\n",
    ")\n",
    "# Resetting the index is crucial to ensure that the DataFrame row number (0, 1, 2...)\n",
    "# matches the ID used in the Faiss/Annoy indexes.\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"Destination data loaded.\")\n",
    "\n",
    "# --- Reusable Helper Function to Generate Embeddings ---\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    pooled_embedding = outputs.hidden_states[-1].mean(dim=1).squeeze()\n",
    "    return pooled_embedding.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f961bf1",
   "metadata": {},
   "source": [
    "RAG System 1 (DeepSeek-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7451fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading tokenizer...\n",
      "Loading fine-tuned model for embedding retrieval...\n",
      "Embedding model loaded successfully.\n",
      "Loading original base model for text generation...\n",
      "Generation model loaded successfully.\n",
      "Loading FAISS index...\n",
      "Loading Annoy index...\n",
      "Loading destination data...\n",
      "Setup complete.\n",
      "\n",
      "--- Starting RAG Recommendation Process with Few-Shot Prompting ---\n",
      "1. Creating few-shot prompt with retrieved context...\n",
      "\n",
      "2. Generating detailed recommendation...\n",
      "\n",
      "✅ --- RAG Recommendation --- ✅\n",
      "[Your Answer]\n",
      "\n",
      "</think>\n",
      "\n",
      "The best recommendation for the user described in the USER PROFILE is Santorini, Greece. This choice aligns with the user's preference for photography and a budget-friendly travel style. Santorini offers a serene environment with breathtaking sunsets and panoramic views, perfect for an authentic photography experience. The destination's cultural heritage and natural beauty make it an ideal choice for a budget-friendly adventure, reflecting the user's interest in cultural experiences and photography.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ===================================================================\n",
    "# PART 1: SETUP - NOW LOADING TWO MODELS\n",
    "# ===================================================================\n",
    "\n",
    "# --- Configuration and Paths ---\n",
    "BASE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "ADAPTER_PATH = \"./fine_tuned_deepseek_travel_prompts\"\n",
    "FAISS_PATH = \"./faiss_index_deepseek.idx\"\n",
    "ANNOY_PATH = \"./annoy_index_deepseek.ann\"\n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "EMBEDDING_DIMENSION = 1536\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Models and Tokenizer ---\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# 1. Load the FINE-TUNED model for EMBEDDINGS\n",
    "print(\"Loading fine-tuned model for embedding retrieval...\")\n",
    "embedding_base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, quantization_config=bnb_config, device_map={\"\": device}, trust_remote_code=True)\n",
    "embedding_model = PeftModel.from_pretrained(embedding_base_model, ADAPTER_PATH)\n",
    "embedding_model.eval()\n",
    "print(\"Embedding model loaded successfully.\")\n",
    "\n",
    "# 2. Load the ORIGINAL PRE-TRAINED model for GENERATION\n",
    "print(\"Loading original base model for text generation...\")\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, quantization_config=bnb_config, device_map={\"\": device}, trust_remote_code=True)\n",
    "generation_model.eval()\n",
    "print(\"Generation model loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Load Indexes and Data (Same as before) ---\n",
    "print(\"Loading FAISS index...\")\n",
    "faiss_index = faiss.read_index(FAISS_PATH)\n",
    "print(\"Loading Annoy index...\")\n",
    "annoy_index = AnnoyIndex(EMBEDDING_DIMENSION, 'angular')\n",
    "annoy_index.load(ANNOY_PATH)\n",
    "print(\"Loading destination data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['destination_description'] = (df['destination_city'] + \" \" + df['destination_country'] + \" \" + df['activities'].fillna('') + \" \" + df['climate'].fillna(''))\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"Setup complete.\")\n",
    "\n",
    "# ===================================================================\n",
    "# PART 2: HELPER AND MAIN FUNCTION DEFINITIONS\n",
    "# ===================================================================\n",
    "\n",
    "# This is the updated RAG function with a few-shot example.\n",
    "# It assumes all the models and data are already loaded from the setup cell.\n",
    "\n",
    "def get_rag_recommendation(user_profile, generation_model, tokenizer, annoy_index, df, num_retrieved=3):\n",
    "    \"\"\"\n",
    "    Generates a RAG-based recommendation using a few-shot prompt to guide the model.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting RAG Recommendation Process with Few-Shot Prompting ---\")\n",
    "\n",
    "    # --- Steps 1 & 2: Retrieve Top-N Relevant Destinations ---\n",
    "    # We still use the fine-tuned embedding model for retrieval\n",
    "    user_prompt_text = (f\"User Profile: Age {user_profile['age']}, {user_profile['gender']}, enjoys {user_profile['hobby']}, budget is {user_profile['budget']}, travel style is {user_profile['travel_style']}.\")\n",
    "    user_embedding = get_embedding(user_prompt_text, embedding_model, tokenizer)\n",
    "    candidate_indices = annoy_index.get_nns_by_vector(user_embedding, num_retrieved, include_distances=False)\n",
    "    retrieved_destinations = df.iloc[candidate_indices]\n",
    "\n",
    "    # --- Step 3: Create the Few-Shot RAG Prompt ---\n",
    "    print(\"1. Creating few-shot prompt with retrieved context...\")\n",
    "    context_string = \"\"\n",
    "    for i, row in retrieved_destinations.iterrows():\n",
    "        context_string += f\"- {row['destination_city']}, {row['destination_country']} (Activities: {row['activities']})\\\\n\"\n",
    "    \n",
    "    # Define a high-quality example to guide the model\n",
    "    few_shot_example = \"\"\"\n",
    "Here is an example of how to solve the task.\n",
    "CONTEXT:\n",
    "- Bali, Indonesia (Activities: Surfing, Yoga)\n",
    "- Banff National Park, Canada (Activities: Hiking, Wildlife)\n",
    "\n",
    "USER PROFILE:\n",
    "User Profile: Age 24, Male, enjoys Hiking, budget is Budget, travel style is Adventure.\n",
    "\n",
    "QUESTION:\n",
    "Based on the provided CONTEXT, which single destination is the best recommendation for the user described in the USER PROFILE? Provide a detailed, paragraph-long answer explaining your choice.\n",
    "\n",
    "ANSWER:\n",
    "The best recommendation is Banff National Park, Canada. The user's passion for hiking and adventure travel aligns perfectly with Banff's world-renowned trails and rugged mountain environment. While Bali offers adventure, its focus on surfing and yoga is less specific to the user's primary hobby of hiking. Banff provides a direct and immersive experience for an adventure-seeking backpacker on a budget.\n",
    "\"\"\"\n",
    "\n",
    "    # Combine the example with the actual problem\n",
    "    rag_prompt = (\n",
    "        f\"{few_shot_example}\\n\\n\"\n",
    "        f\"----------------------------------\\n\\n\"\n",
    "        f\"Now, solve this new problem:\\n\\n\"\n",
    "        f\"CONTEXT:\\n{context_string}\\n\"\n",
    "        f\"USER PROFILE:\\n{user_prompt_text}\\n\\n\"\n",
    "        f\"QUESTION:\\n\"\n",
    "        f\"Based on the provided CONTEXT, which single destination is the best recommendation for the user described in the USER PROFILE? Provide a detailed, paragraph-long answer explaining your choice.\\n\\n\"\n",
    "        f\"ANSWER:\"\n",
    "    )\n",
    "    \n",
    "    # --- Step 4: Generate the Final Answer ---\n",
    "    print(\"\\n2. Generating detailed recommendation...\")\n",
    "    inputs = tokenizer(rag_prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = generation_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=250, # Increased token limit for the answer\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.5, # A bit of creativity but still focused\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9\n",
    "        )\n",
    "            \n",
    "    full_generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    # The answer is everything after the final \"ANSWER:\" marker in the prompt\n",
    "    answer_marker = \"ANSWER:\"\n",
    "    # Find the last occurrence of the marker to get the final answer\n",
    "    answer_start_index = full_generated_text.rfind(answer_marker)\n",
    "    \n",
    "    if answer_start_index != -1:\n",
    "        clean_answer = full_generated_text[answer_start_index + len(answer_marker):].strip()\n",
    "    else:\n",
    "        clean_answer = \"Could not parse the final answer from the model's response.\"\n",
    "\n",
    "    return clean_answer\n",
    "\n",
    "# --- You will also need to update the final function call to remove the models we aren't using in this function ---\n",
    "# Note: The main execution code block needs to be updated slightly\n",
    "\n",
    "# final_recommendations = get_rag_recommendation(\n",
    "#     new_user,\n",
    "#     embedding_model=embedding_model, # Still needed for get_embedding helper\n",
    "#     generation_model=generation_model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     annoy_index=annoy_index,\n",
    "#     # faiss_index is no longer used in this simplified RAG\n",
    "#     df=df\n",
    "# )\n",
    "# ===================================================================\n",
    "# PART 3: EXECUTION (Final Version)\n",
    "# ===================================================================\n",
    "new_user = {\"age\": 32, \"gender\": \"Female\", \"hobby\": \"Photography\", \"budget\": \"Luxury\", \"travel_style\": \"Cultural\"}\n",
    "\n",
    "# Get and print the RAG recommendation\n",
    "final_recommendations = get_rag_recommendation(\n",
    "    new_user, \n",
    "    generation_model=generation_model,\n",
    "    tokenizer=tokenizer, \n",
    "    annoy_index=annoy_index, \n",
    "    df=df\n",
    ")\n",
    "\n",
    "print(\"\\n✅ --- RAG Recommendation --- ✅\")\n",
    "print(final_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056c922",
   "metadata": {},
   "source": [
    "RAG System 2 (BERT-based)\n",
    "Code for BERT-based RAG System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6433d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading fine-tuned BERT model for embedding retrieval...\n",
      "BERT embedding model loaded successfully.\n",
      "Loading Annoy index from: ./annoy_index_bert.ann\n",
      "Loading destination data...\n",
      "Data setup complete.\n",
      "\n",
      "--- Performing Retrieval with BERT ---\n",
      "Retrieval step complete. RAG prompt has been created (or error was flagged).\n",
      "\n",
      "--- Clearing memory before loading generation model ---\n",
      "Memory cleared.\n",
      "\n",
      "--- Loading original DeepSeek base model for text generation ---\n",
      "Generation model loaded successfully.\n",
      "\n",
      "--- Generating final recommendation ---\n",
      "\n",
      "✅ --- BERT-RAG Recommendation --- ✅\n",
      "A user with the profile 'User Profile: Age 32, Female, enjoys Photography, budget is Luxury, travel style is Cultural.' is considering these destinations:\\n- Bali, Indonesia (Activities: Surfing, Relaxing)\\n- Banff, Canada (Activities: Lake Trails, Snow Hiking)\\n- Berlin, Germany (Activities: Electronic Music, Nightlife)\\n\\nBased on this information, which one destination is the best recommendation? Explain why in a detailed paragraph. You should not use any markdown, and each paragraph should be ...\\n\\n**Answer\\n\\nThe best recommendation is Bali because it offers the most activities and is accessible from the user's profile. Bali is located in Indonesia, which is a country with a vibrant culture and diverse landscapes. The activities listed, surfing and relaxing, align with the user's interests in photography and a budget lifestyle. Additionally, Bali's proximity to various attractions makes it easy for the user to plan their trip without being too far from important spots. The cultural aspect of Bali also aligns with the user's enjoyment of photography and a relaxed lifestyle. Therefore, Bali is the optimal choice for the user's profile and destination considerations.\n",
      "</think>\n",
      "\n",
      "The best recommendation is Bali because it offers the most activities and is accessible from the user's profile. Bali is located in Indonesia, which is a country with a vibrant culture and diverse landscapes. The activities listed, surfing and relaxing, align with the user's interests in photography and a budget lifestyle. Additionally, Bali's proximity to various attractions makes it easy for the user to plan their trip without being too far from important spots. The cultural aspect of Bali also aligns with the user's enjoyment of photography and a relaxed lifestyle. Therefore, Bali is the optimal choice for the user's profile and destination considerations.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, BertForSequenceClassification, BertTokenizer\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ===================================================================\n",
    "# PART 1: RETRIEVAL (using Fine-Tuned BERT)\n",
    "# ===================================================================\n",
    "\n",
    "# --- Configuration and Paths ---\n",
    "BERT_ADAPTER_PATH = \"./fine_tuned_bert_travel\"\n",
    "DEEPSEEK_BASE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# This path needs to point to an Annoy index built with BERT embeddings.\n",
    "# For now, we point to the DeepSeek one to demonstrate the dimension mismatch.\n",
    "ANNOY_PATH = \"./annoy_index_bert.ann\"\n",
    "# This dimension must match the Annoy index file. BERT's is 768, but we use 1536 to match the file.\n",
    "EMBEDDING_DIMENSION = 768\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load ONLY the Fine-Tuned BERT Model and its Tokenizer ---\n",
    "print(\"Loading fine-tuned BERT model for embedding retrieval...\")\n",
    "embedding_model_bert = BertForSequenceClassification.from_pretrained(BERT_ADAPTER_PATH)\n",
    "embedding_model_bert.to(device)\n",
    "embedding_model_bert.eval()\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(BERT_ADAPTER_PATH)\n",
    "print(\"BERT embedding model loaded successfully.\")\n",
    "\n",
    "# --- Load Index and Data ---\n",
    "print(f\"Loading Annoy index from: {ANNOY_PATH}\")\n",
    "annoy_index = AnnoyIndex(EMBEDDING_DIMENSION, 'angular')\n",
    "annoy_index.load(ANNOY_PATH)\n",
    "print(\"Loading destination data...\")\n",
    "df = pd.read_csv('travel_recommendations_10k.csv')\n",
    "df['destination_description'] = (df['destination_city'] + \" \" + df['destination_country'] + \" \" + df['activities'].fillna('') + \" \" + df['climate'].fillna(''))\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"Data setup complete.\")\n",
    "\n",
    "# --- Define User and Helper Function ---\n",
    "new_user = {\"age\": 32, \"gender\": \"Female\", \"hobby\": \"Photography\", \"budget\": \"Luxury\", \"travel_style\": \"Cultural\"}\n",
    "\n",
    "def get_embedding_from_bert(text, model, tokenizer):\n",
    "    \"\"\"Generates an embedding from a fine-tuned BERT model.\"\"\"\n",
    "    device = model.device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**inputs, output_hidden_states=True)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    pooled_embedding = last_hidden_state.mean(dim=1).squeeze()\n",
    "    return pooled_embedding.cpu().numpy()\n",
    "\n",
    "# --- Perform the Retrieval Step ---\n",
    "print(\"\\n--- Performing Retrieval with BERT ---\")\n",
    "user_prompt_text = (f\"User Profile: Age {new_user['age']}, {new_user['gender']}, enjoys {new_user['hobby']}, budget is {new_user['budget']}, travel style is {new_user['travel_style']}.\")\n",
    "user_embedding = get_embedding_from_bert(user_prompt_text, embedding_model_bert, tokenizer_bert)\n",
    "\n",
    "# --- Check for Dimension Mismatch ---\n",
    "# This check will now run correctly and print the error message as intended.\n",
    "if user_embedding.shape[0] != EMBEDDING_DIMENSION:\n",
    "    print(f\"\\nCRITICAL ERROR: Dimension mismatch.\")\n",
    "    print(f\"BERT embedding dimension is {user_embedding.shape[0]}, but the loaded Annoy index requires dimension {EMBEDDING_DIMENSION}.\")\n",
    "    print(\"To fix this, you must first generate embeddings for ALL destinations using your fine-tuned BERT model and build a new Annoy index from them.\")\n",
    "    rag_prompt = \"Error: Dimension Mismatch\" # Set prompt to error to prevent next step\n",
    "else:\n",
    "    candidate_indices = annoy_index.get_nns_by_vector(user_embedding, 3, include_distances=False)\n",
    "    retrieved_destinations = df.iloc[candidate_indices]\n",
    "    context_string = \"\"\n",
    "    for i, row in retrieved_destinations.iterrows():\n",
    "        context_string += f\"- {row['destination_city']}, {row['destination_country']} (Activities: {row['activities']})\\\\n\"\n",
    "    rag_prompt = (\n",
    "        f\"A user with the profile '{user_prompt_text}' is considering these destinations:\\\\n{context_string}\\\\n\"\n",
    "        f\"Based on this information, which one destination is the best recommendation? Explain why in a detailed paragraph.\"\n",
    "    )\n",
    "print(\"Retrieval step complete. RAG prompt has been created (or error was flagged).\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PART 2: CLEAR GPU MEMORY\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n--- Clearing memory before loading generation model ---\")\n",
    "del embedding_model_bert\n",
    "del tokenizer_bert\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PART 3: GENERATION (using the Original DeepSeek Model)\n",
    "# ===================================================================\n",
    "\n",
    "if \"Error\" not in rag_prompt:\n",
    "    print(\"\\n--- Loading original DeepSeek base model for text generation ---\")\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    generation_model = AutoModelForCausalLM.from_pretrained(\n",
    "        DEEPSEEK_BASE_MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": device},\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    generation_model.eval()\n",
    "    tokenizer_deepseek = AutoTokenizer.from_pretrained(DEEPSEEK_BASE_MODEL_NAME, trust_remote_code=True)\n",
    "    if tokenizer_deepseek.pad_token is None:\n",
    "        tokenizer_deepseek.pad_token = tokenizer_deepseek.eos_token\n",
    "    print(\"Generation model loaded successfully.\")\n",
    "\n",
    "    # --- Generate the Final Answer ---\n",
    "    print(\"\\n--- Generating final recommendation ---\")\n",
    "    inputs = tokenizer_deepseek(rag_prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = generation_model.generate(**inputs, max_new_tokens=350, pad_token_id=tokenizer_deepseek.eos_token_id)\n",
    "    clean_answer = tokenizer_deepseek.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n✅ --- BERT-RAG Recommendation --- ✅\")\n",
    "    print(clean_answer)\n",
    "else:\n",
    "    print(\"\\n--- Generation Skipped ---\")\n",
    "    print(\"Could not proceed due to the dimension mismatch error identified in the retrieval step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bdafa7",
   "metadata": {},
   "source": [
    " now generating index using bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e9376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing Data and Model for BERT Embedding Generation ---\n",
      "Using device: cuda\n",
      "Loaded and prepared 8000 training samples.\n",
      "Loading fine-tuned BERT model...\n",
      "BERT model loaded successfully.\n",
      "\n",
      "--- Generating BERT Embeddings for All Destinations ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Destinations: 100%|██████████| 8000/8000 [01:23<00:00, 96.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation complete. Matrix shape: (8000, 768)\n",
      "\n",
      "--- Building FAISS Index with BERT Embeddings ---\n",
      "FAISS index built and saved to 'faiss_index_bert.idx'\n",
      "\n",
      "--- Building Annoy Index with BERT Embeddings ---\n",
      "Annoy index built and saved to 'annoy_index_bert.ann'\n",
      "\n",
      "--- New BERT-based indexes are ready. ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# SETUP\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Preparing Data and Model for BERT Embedding Generation ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BERT_ADAPTER_PATH = \"./fine_tuned_bert_travel\"\n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "BERT_EMBEDDING_DIM = 768 # Standard for BERT-base models\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Data ---\n",
    "# We need the training portion of the data that corresponds to the indexes\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['destination_description'] = (df['destination_city'] + \" \" + df['destination_country'] + \" \" + df['activities'].fillna('') + \" \" + df['climate'].fillna(''))\n",
    "train_df, _ = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Loaded and prepared {len(train_df)} training samples.\")\n",
    "\n",
    "# --- Load Fine-Tuned BERT Model ---\n",
    "print(\"Loading fine-tuned BERT model...\")\n",
    "embedding_model_bert = BertForSequenceClassification.from_pretrained(BERT_ADAPTER_PATH)\n",
    "embedding_model_bert.to(device)\n",
    "embedding_model_bert.eval()\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(BERT_ADAPTER_PATH)\n",
    "print(\"BERT model loaded successfully.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# EMBEDDING GENERATION\n",
    "# ===================================================================\n",
    "\n",
    "def get_embedding_from_bert(text, model, tokenizer):\n",
    "    \"\"\"Generates an embedding from a fine-tuned BERT model.\"\"\"\n",
    "    device = model.device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**inputs, output_hidden_states=True)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    pooled_embedding = last_hidden_state.mean(dim=1).squeeze()\n",
    "    return pooled_embedding.cpu().numpy()\n",
    "\n",
    "print(\"\\n--- Generating BERT Embeddings for All Destinations ---\")\n",
    "bert_embeddings = []\n",
    "for description in tqdm(train_df['destination_description'], desc=\"Processing Destinations\"):\n",
    "    embedding = get_embedding_from_bert(description, embedding_model_bert, tokenizer_bert)\n",
    "    bert_embeddings.append(embedding)\n",
    "\n",
    "embedding_matrix_bert = np.array(bert_embeddings).astype('float32')\n",
    "print(f\"Embedding generation complete. Matrix shape: {embedding_matrix_bert.shape}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# INDEX BUILDING\n",
    "# ===================================================================\n",
    "\n",
    "# --- Build and Save New Faiss Index ---\n",
    "print(\"\\n--- Building FAISS Index with BERT Embeddings ---\")\n",
    "faiss_index_bert = faiss.IndexFlatL2(BERT_EMBEDDING_DIM)\n",
    "faiss_index_bert.add(embedding_matrix_bert)\n",
    "faiss.write_index(faiss_index_bert, 'faiss_index_bert.idx')\n",
    "print(f\"FAISS index built and saved to 'faiss_index_bert.idx'\")\n",
    "\n",
    "# --- Build and Save New Annoy Index ---\n",
    "print(\"\\n--- Building Annoy Index with BERT Embeddings ---\")\n",
    "annoy_index_bert = AnnoyIndex(BERT_EMBEDDING_DIM, 'angular')\n",
    "for i in range(len(embedding_matrix_bert)):\n",
    "    annoy_index_bert.add_item(i, embedding_matrix_bert[i])\n",
    "annoy_index_bert.build(10)\n",
    "annoy_index_bert.save('annoy_index_bert.ann')\n",
    "print(f\"Annoy index built and saved to 'annoy_index_bert.ann'\")\n",
    "\n",
    "print(\"\\n--- New BERT-based indexes are ready. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8b139",
   "metadata": {},
   "source": [
    "@RAG System 3 (MiniLM-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c74ef",
   "metadata": {},
   "source": [
    "Step 1: Code to Generate Fine-Tuned MiniLM Embeddings and Indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c81f773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ausaf raza\\anaconda3\\envs\\travel_recommender_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "--- Preparing Data and Model for Fine-Tuned MiniLM Embedding Generation ---\n",
      "Using device: cuda\n",
      "Loaded and prepared 8000 training samples.\n",
      "Loading fine-tuned MiniLM model from: ./fine_tuned_minilm_travel\n",
      "Fine-tuned MiniLM model loaded successfully.\n",
      "\n",
      "--- Generating Fine-Tuned MiniLM Embeddings for All Destinations ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 250/250 [00:03<00:00, 63.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation complete. Matrix shape: (8000, 384)\n",
      "\n",
      "--- Building FAISS Index with Fine-Tuned MiniLM Embeddings ---\n",
      "FAISS index built and saved to 'faiss_index_minilm_finetuned.idx'\n",
      "\n",
      "--- Building Annoy Index with Fine-Tuned MiniLM Embeddings ---\n",
      "Annoy index built and saved to 'annoy_index_minilm_finetuned.ann'\n",
      "\n",
      "--- New MiniLM-based indexes are ready. ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# SETUP\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Preparing Data and Model for Fine-Tuned MiniLM Embedding Generation ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "MINILM_FINETUNED_PATH = \"./fine_tuned_minilm_travel\"\n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "MINILM_EMBEDDING_DIM = 384 # Standard for this MiniLM model\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['destination_description'] = (df['destination_city'] + \" \" + df['destination_country'] + \" \" + df['activities'].fillna('') + \" \" + df['climate'].fillna(''))\n",
    "train_df, _ = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Loaded and prepared {len(train_df)} training samples.\")\n",
    "\n",
    "# --- Load Fine-Tuned MiniLM Model ---\n",
    "print(f\"Loading fine-tuned MiniLM model from: {MINILM_FINETUNED_PATH}\")\n",
    "embedding_model_minilm = SentenceTransformer(MINILM_FINETUNED_PATH)\n",
    "embedding_model_minilm.to(device)\n",
    "print(\"Fine-tuned MiniLM model loaded successfully.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# EMBEDDING GENERATION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n--- Generating Fine-Tuned MiniLM Embeddings for All Destinations ---\")\n",
    "# The .encode method is highly optimized for batch processing\n",
    "minilm_embeddings = embedding_model_minilm.encode(\n",
    "    train_df['destination_description'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "embedding_matrix_minilm = np.array(minilm_embeddings).astype('float32')\n",
    "print(f\"Embedding generation complete. Matrix shape: {embedding_matrix_minilm.shape}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# INDEX BUILDING\n",
    "# ===================================================================\n",
    "\n",
    "# --- Build and Save New Faiss Index ---\n",
    "print(\"\\n--- Building FAISS Index with Fine-Tuned MiniLM Embeddings ---\")\n",
    "faiss_index_minilm = faiss.IndexFlatL2(MINILM_EMBEDDING_DIM)\n",
    "faiss_index_minilm.add(embedding_matrix_minilm)\n",
    "# Save with a new name to avoid overwriting your original indexes\n",
    "faiss.write_index(faiss_index_minilm, 'faiss_index_minilm_finetuned.idx')\n",
    "print(f\"FAISS index built and saved to 'faiss_index_minilm_finetuned.idx'\")\n",
    "\n",
    "# --- Build and Save New Annoy Index ---\n",
    "print(\"\\n--- Building Annoy Index with Fine-Tuned MiniLM Embeddings ---\")\n",
    "annoy_index_minilm = AnnoyIndex(MINILM_EMBEDDING_DIM, 'angular')\n",
    "for i in range(len(embedding_matrix_minilm)):\n",
    "    annoy_index_minilm.add_item(i, embedding_matrix_minilm[i])\n",
    "annoy_index_minilm.build(10)\n",
    "# Save with a new name\n",
    "annoy_index_minilm.save('annoy_index_minilm_finetuned.ann')\n",
    "print(f\"Annoy index built and saved to 'annoy_index_minilm_finetuned.ann'\")\n",
    "\n",
    "print(\"\\n--- New MiniLM-based indexes are ready. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337593a",
   "metadata": {},
   "source": [
    "Code for MiniLM-based RAG System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a710a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading fine-tuned MiniLM model for embedding retrieval...\n",
      "MiniLM embedding model loaded successfully.\n",
      "Loading original DeepSeek base model for text generation...\n",
      "DeepSeek generation model loaded successfully.\n",
      "Loading Annoy index from: ./annoy_index_minilm_finetuned.ann\n",
      "Loading destination data...\n",
      "Setup complete.\n",
      "\n",
      "--- Starting MiniLM-RAG Recommendation Process ---\n",
      "\n",
      "✅ --- MiniLM-RAG Recommendation --- ✅\n",
      "You should not use any markdown, should be concise (under 500 words), and use clear language.\n",
      "Okay, so I need to help this user choose the best destination based on their profile and the activities they enjoy. Let me break this down.\n",
      "\n",
      "The user is 32, female, into photography, and they have a budget for luxury. Their interests are in travel style as Cultural and enjoy activities like beach parties and snorkeling. The destinations are Berlin, Cancun, and Bali.\n",
      "\n",
      "Starting with Berlin, Germany. The activities listed are Electronic Music and Nightlife. Hmm, I'm not super familiar with Electronic Music in Berlin, but nightlife is definitely a key part of Berlin's vibe. It's known for its nightlife, clubs, and parties. But does that fit into their budget for luxury? Maybe not, since nightlife can be more casual and not as luxurious as some other destinations.\n",
      "\n",
      "Next, Cancun, Mexico. The activities here are Beach Parties and Snorkeling. Beach parties are definitely a thing in Cancun, known for its nightlife and party culture. Snorkeling is more of an adventure, which might be more for someone who loves the outdoors. However, Cancun is known for its luxury, especially in resorts and the experience of escaping the city. The beach is a big part of Cancun's appeal, so maybe this is a good fit considering their budget for luxury.\n",
      "\n",
      "Lastly, Bali, Indonesia. The activities are Surfing and Relaxing. Bali is famous for its surf spots and surf schools, which is a great way to enjoy the beach. Relaxing is another big aspect of Bali's culture, especially in the temples and beaches. The culture and relaxation are definitely part of Bali's appeal,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ===================================================================\n",
    "# PART 1: SETUP for MiniLM-RAG\n",
    "# ===================================================================\n",
    "\n",
    "# --- Configuration and Paths ---\n",
    "MINILM_FINETUNED_PATH = \"./fine_tuned_minilm_travel\"\n",
    "DEEPSEEK_BASE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "ANNOY_PATH_MINILM = \"./annoy_index_minilm_finetuned.ann\" \n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "MINILM_EMBEDDING_DIM = 384 # Dimension for MiniLM\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Models and Tokenizer ---\n",
    "print(\"Loading fine-tuned MiniLM model for embedding retrieval...\")\n",
    "# 1. Load the FINE-TUNED MiniLM model for EMBEDDINGS\n",
    "embedding_model_minilm = SentenceTransformer(MINILM_FINETUNED_PATH)\n",
    "embedding_model_minilm.to(device)\n",
    "print(\"MiniLM embedding model loaded successfully.\")\n",
    "\n",
    "# 2. Load the ORIGINAL DEEPSEEK model for GENERATION\n",
    "print(\"Loading original DeepSeek base model for text generation...\")\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEEPSEEK_BASE_MODEL_NAME, \n",
    "    quantization_config=bnb_config, \n",
    "    device_map={\"\": device}, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "generation_model.eval()\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(DEEPSEEK_BASE_MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer_deepseek.pad_token is None:\n",
    "    tokenizer_deepseek.pad_token = tokenizer_deepseek.eos_token\n",
    "print(\"DeepSeek generation model loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Load Indexes and Data ---\n",
    "print(f\"Loading Annoy index from: {ANNOY_PATH_MINILM}\")\n",
    "annoy_index = AnnoyIndex(MINILM_EMBEDDING_DIM, 'angular')\n",
    "annoy_index.load(ANNOY_PATH_MINILM)\n",
    "print(\"Loading destination data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['destination_description'] = (df['destination_city'] + \" \" + df['destination_country'] + \" \" + df['activities'].fillna('') + \" \" + df['climate'].fillna(''))\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"Setup complete.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PART 2: HELPER AND MAIN FUNCTION DEFINITIONS for MiniLM-RAG\n",
    "# ===================================================================\n",
    "\n",
    "def get_embedding_from_minilm(text, model):\n",
    "    \"\"\"Generates an embedding from a fine-tuned SentenceTransformer model.\"\"\"\n",
    "    # .encode is the standard method for SentenceTransformer models\n",
    "    embedding = model.encode(text, convert_to_numpy=True)\n",
    "    return embedding\n",
    "\n",
    "# Main RAG function adapted for MiniLM retrieval\n",
    "def get_minilm_rag_recommendation(user_profile, embedding_model_minilm, generation_model, tokenizer_deepseek, annoy_index, df, num_retrieved=3):\n",
    "    print(\"\\n--- Starting MiniLM-RAG Recommendation Process ---\")\n",
    "\n",
    "    # Step 1: Use FINE-TUNED MiniLM to get the user embedding\n",
    "    user_prompt_text = (f\"User Profile: Age {user_profile['age']}, {user_profile['gender']}, enjoys {user_profile['hobby']}, budget is {user_profile['budget']}, travel style is {user_profile['travel_style']}.\")\n",
    "    user_embedding = get_embedding_from_minilm(user_prompt_text, embedding_model_minilm)\n",
    "\n",
    "    # Step 2: Retrieve destinations\n",
    "    candidate_indices = annoy_index.get_nns_by_vector(user_embedding, num_retrieved, include_distances=False)\n",
    "    retrieved_destinations = df.iloc[candidate_indices]\n",
    "    \n",
    "    # Step 3: Create RAG prompt\n",
    "    context_string = \"\"\n",
    "    for i, row in retrieved_destinations.iterrows():\n",
    "        context_string += f\"- {row['destination_city']}, {row['destination_country']} (Activities: {row['activities']})\\\\n\"\n",
    "    rag_prompt = (f\"A user with the profile '{user_prompt_text}' is considering these destinations:\\\\n{context_string}\\\\nBased on this information, which one destination is the best recommendation? Explain why in a detailed paragraph.\")\n",
    "    \n",
    "    # Step 4: Use DeepSeek base model to generate the answer\n",
    "    inputs = tokenizer_deepseek(rag_prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = generation_model.generate(**inputs, max_new_tokens=350, pad_token_id=tokenizer_deepseek.eos_token_id)\n",
    "    generated_text = tokenizer_deepseek.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Parse the output\n",
    "    answer_marker = \"Explain why in a detailed paragraph.\"\n",
    "    answer_start_index = generated_text.find(answer_marker)\n",
    "    if answer_start_index != -1:\n",
    "        clean_answer = generated_text[answer_start_index + len(answer_marker):].strip()\n",
    "    else:\n",
    "        clean_answer = generated_text # Fallback\n",
    "    \n",
    "    return clean_answer\n",
    "\n",
    "# ===================================================================\n",
    "# PART 3: EXECUTION\n",
    "# ===================================================================\n",
    "new_user = {\"age\": 32, \"gender\": \"Female\", \"hobby\": \"Photography\", \"budget\": \"Luxury\", \"travel_style\": \"Cultural\"}\n",
    "\n",
    "final_recommendations = get_minilm_rag_recommendation(\n",
    "    new_user,\n",
    "    embedding_model_minilm=embedding_model_minilm,\n",
    "    generation_model=generation_model,\n",
    "    tokenizer_deepseek=tokenizer_deepseek,\n",
    "    annoy_index=annoy_index,\n",
    "    df=df\n",
    ")\n",
    "\n",
    "print(\"\\n✅ --- MiniLM-RAG Recommendation --- ✅\")\n",
    "print(final_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604485c",
   "metadata": {},
   "source": [
    "## @@Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bee09",
   "metadata": {},
   "source": [
    "### for non-rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "533a2a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing Models and Data for Evaluation ---\n",
      "Models and data ready for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Top-10: 100%|██████████| 2000/2000 [00:17<00:00, 113.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results for Fine-Tuned MiniLM (non-RAG) ---\n",
      "Hit Rate@10: 0.2195\n",
      "Precision@10: 0.0220\n",
      "Recall@10: 0.2195\n",
      "MAP@10: 0.2195\n",
      "NDCG@10: 0.2195\n",
      "\n",
      "--- Distribution of Hit Ranks ---\n",
      "Rank 1: 439 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter # Import Counter to easily tabulate ranks\n",
    "\n",
    "# ===================================================================\n",
    "# PART 1: SETUP (Same as before)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Preparing Models and Data for Evaluation ---\")\n",
    "# ... (The setup code for loading models and data is identical to the last step) ...\n",
    "# --- Configuration ---\n",
    "MINILM_FINETUNED_PATH = \"./fine_tuned_minilm_travel\"\n",
    "FAISS_INDEX_PATH = \"./faiss_index_minilm_finetuned.idx\"\n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# --- Load Data and Create Key Mapping ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['destination_key'] = df['destination_city'] + '__' + df['destination_country']\n",
    "df['user_profile_text'] = (df['age'].astype(str) + \" \" + df['gender'].astype(str).fillna('') + \" \" + df['hobby'].fillna('') + \" \" + df['budget'].fillna('') + \" \" + df['travel_style'].fillna(''))\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "index_to_destination_key = pd.Series(train_df['destination_key'].values, index=train_df.index)\n",
    "# --- Load Fine-Tuned MiniLM Model and FAISS Index ---\n",
    "embedding_model = SentenceTransformer(MINILM_FINETUNED_PATH)\n",
    "embedding_model.to(device)\n",
    "faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "print(\"Models and data ready for evaluation.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PART 2: FINAL EVALUATION SCRIPT\n",
    "# ===================================================================\n",
    "\n",
    "def evaluate_retrieval_system(test_data, model, faiss_idx, index_map, k=10):\n",
    "    \"\"\"\n",
    "    Calculates ranking metrics and the distribution of hit ranks.\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    precision_at_k_sum = 0\n",
    "    recall_at_k_sum = 0\n",
    "    map_sum = 0\n",
    "    ndcg_sum = 0\n",
    "    hit_ranks = [] # NEW: Initialize list to store ranks of hits\n",
    "    \n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=f\"Evaluating Top-{k}\"):\n",
    "        user_profile = row['user_profile_text']\n",
    "        ground_truth_item_key = row['destination_key']\n",
    "\n",
    "        user_embedding = model.encode(user_profile, convert_to_numpy=True).reshape(1, -1)\n",
    "        _, top_k_indices = faiss_idx.search(user_embedding, k)\n",
    "        recommended_item_keys = [index_map.get(i) for i in top_k_indices[0]]\n",
    "        \n",
    "        if ground_truth_item_key in recommended_item_keys:\n",
    "            hits += 1\n",
    "            precision_at_k_sum += 1 / k\n",
    "            recall_at_k_sum += 1 \n",
    "            \n",
    "            rank = recommended_item_keys.index(ground_truth_item_key) + 1\n",
    "            hit_ranks.append(rank) # NEW: Record the rank of the successful hit\n",
    "            \n",
    "            map_sum += 1 / rank\n",
    "            ndcg_sum += 1 / np.log2(rank + 1)\n",
    "\n",
    "    # Calculate final average metrics\n",
    "    hit_rate = hits / len(test_data) if len(test_data) > 0 else 0\n",
    "    avg_precision = precision_at_k_sum / len(test_data) if len(test_data) > 0 else 0\n",
    "    avg_recall = recall_at_k_sum / len(test_data) if len(test_data) > 0 else 0\n",
    "    mean_avg_precision = map_sum / len(test_data) if len(test_data) > 0 else 0\n",
    "    avg_ndcg = ndcg_sum / len(test_data) if len(test_data) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'metrics': {\n",
    "            f'Hit Rate@{k}': hit_rate,\n",
    "            f'Precision@{k}': avg_precision,\n",
    "            f'Recall@{k}': avg_recall,\n",
    "            f'MAP@{k}': mean_avg_precision,\n",
    "            f'NDCG@{k}': avg_ndcg\n",
    "        },\n",
    "        'hit_rank_distribution': Counter(hit_ranks) # NEW: Return the rank distribution\n",
    "    }\n",
    "\n",
    "# ===================================================================\n",
    "# PART 3: EXECUTION AND RESULTS\n",
    "# ===================================================================\n",
    "\n",
    "TOP_K = 10\n",
    "results = evaluate_retrieval_system(\n",
    "    test_data=test_df,\n",
    "    model=embedding_model,\n",
    "    faiss_idx=faiss_index,\n",
    "    index_map=index_to_destination_key,\n",
    "    k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluation Results for Fine-Tuned MiniLM (non-RAG) ---\")\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n--- Distribution of Hit Ranks ---\")\n",
    "# Sort the ranks for a cleaner display\n",
    "sorted_ranks = sorted(results['hit_rank_distribution'].items())\n",
    "for rank, count in sorted_ranks:\n",
    "    print(f\"Rank {rank}: {count} hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb7d50",
   "metadata": {},
   "source": [
    "### Code to Evaluate the Fine-Tuned BERT System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa3ba492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing Models and Data for BERT Evaluation ---\n",
      "Using device: cuda\n",
      "Loaded and prepared 8000 training and 2000 testing samples.\n",
      "Loading fine-tuned BERT model from: ./fine_tuned_bert_travel\n",
      "Loading FAISS index from: ./faiss_index_bert.idx\n",
      "Models and data ready for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Top-10: 100%|██████████| 2000/2000 [00:25<00:00, 79.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results for Fine-Tuned BERT (non-RAG) ---\n",
      "Hit Rate@10: 0.0410\n",
      "Precision@10: 0.0041\n",
      "Recall@10: 0.0410\n",
      "MAP@10: 0.0410\n",
      "NDCG@10: 0.0410\n",
      "\n",
      "--- Distribution of Hit Ranks ---\n",
      "Rank 1: 82 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ===================================================================\n",
    "# PART 1: SETUP FOR BERT EVALUATION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Preparing Models and Data for BERT Evaluation ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BERT_FINETUNED_PATH = \"./fine_tuned_bert_travel\"\n",
    "FAISS_INDEX_PATH = \"./faiss_index_bert.idx\" # Use the BERT-specific index\n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "BERT_EMBEDDING_DIM = 768 # Dimension for BERT\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Data and Create Key Mapping ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['destination_key'] = df['destination_city'] + '__' + df['destination_country']\n",
    "df['user_profile_text'] = (df['age'].astype(str) + \" \" + df['gender'].astype(str).fillna('') + \" \" + df['hobby'].fillna('') + \" \" + df['budget'].fillna('') + \" \" + df['travel_style'].fillna(''))\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "index_to_destination_key = pd.Series(train_df['destination_key'].values, index=train_df.index)\n",
    "print(f\"Loaded and prepared {len(train_df)} training and {len(test_df)} testing samples.\")\n",
    "\n",
    "# --- Load Fine-Tuned BERT Model and FAISS Index ---\n",
    "print(f\"Loading fine-tuned BERT model from: {BERT_FINETUNED_PATH}\")\n",
    "embedding_model = BertForSequenceClassification.from_pretrained(BERT_FINETUNED_PATH)\n",
    "embedding_model.to(device)\n",
    "embedding_model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_FINETUNED_PATH)\n",
    "\n",
    "print(f\"Loading FAISS index from: {FAISS_INDEX_PATH}\")\n",
    "faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "print(\"Models and data ready for evaluation.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PART 2: HELPER FUNCTION AND EVALUATION SCRIPT\n",
    "# ===================================================================\n",
    "\n",
    "def get_embedding_from_bert(text, model, tokenizer):\n",
    "    \"\"\"Generates an embedding from a fine-tuned BERT model.\"\"\"\n",
    "    device = model.device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        # Get the hidden states from the base BERT model\n",
    "        outputs = model.bert(**inputs, output_hidden_states=True)\n",
    "    # Take the last hidden state and average across the tokens\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    pooled_embedding = last_hidden_state.mean(dim=1).squeeze()\n",
    "    return pooled_embedding.cpu().numpy()\n",
    "\n",
    "# This evaluation function is the same as before\n",
    "def evaluate_retrieval_system(test_data, model, tokenizer, faiss_idx, index_map, k=10):\n",
    "    \"\"\"\n",
    "    Calculates ranking metrics and the distribution of hit ranks.\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    map_sum = 0\n",
    "    ndcg_sum = 0\n",
    "    hit_ranks = []\n",
    "    \n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=f\"Evaluating Top-{k}\"):\n",
    "        user_profile = row['user_profile_text']\n",
    "        ground_truth_item_key = row['destination_key']\n",
    "\n",
    "        # Use the specific embedding function for BERT\n",
    "        user_embedding = get_embedding_from_bert(user_profile, model, tokenizer).reshape(1, -1)\n",
    "        _, top_k_indices = faiss_idx.search(user_embedding, k)\n",
    "        recommended_item_keys = [index_map.get(i) for i in top_k_indices[0]]\n",
    "        \n",
    "        if ground_truth_item_key in recommended_item_keys:\n",
    "            hits += 1\n",
    "            rank = recommended_item_keys.index(ground_truth_item_key) + 1\n",
    "            hit_ranks.append(rank)\n",
    "            map_sum += 1 / rank\n",
    "            ndcg_sum += 1 / np.log2(rank + 1)\n",
    "\n",
    "    num_test_samples = len(test_data)\n",
    "    hit_rate = hits / num_test_samples\n",
    "    mean_avg_precision = map_sum / num_test_samples\n",
    "    avg_ndcg = ndcg_sum / num_test_samples\n",
    "\n",
    "    return {\n",
    "        'metrics': {\n",
    "            f'Hit Rate@{k}': hit_rate,\n",
    "            f'Precision@{k}': hit_rate / k, # Simplified calculation\n",
    "            f'Recall@{k}': hit_rate, # Simplified calculation\n",
    "            f'MAP@{k}': mean_avg_precision,\n",
    "            f'NDCG@{k}': avg_ndcg\n",
    "        },\n",
    "        'hit_rank_distribution': Counter(hit_ranks)\n",
    "    }\n",
    "\n",
    "# ===================================================================\n",
    "# PART 3: EXECUTION AND RESULTS\n",
    "# ===================================================================\n",
    "\n",
    "TOP_K = 10\n",
    "results = evaluate_retrieval_system(\n",
    "    test_data=test_df,\n",
    "    model=embedding_model,\n",
    "    tokenizer=tokenizer,\n",
    "    faiss_idx=faiss_index,\n",
    "    index_map=index_to_destination_key,\n",
    "    k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluation Results for Fine-Tuned BERT (non-RAG) ---\")\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n--- Distribution of Hit Ranks ---\")\n",
    "sorted_ranks = sorted(results['hit_rank_distribution'].items())\n",
    "if not sorted_ranks:\n",
    "    print(\"No hits were recorded.\")\n",
    "for rank, count in sorted_ranks:\n",
    "    print(f\"Rank {rank}: {count} hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159dc13f",
   "metadata": {},
   "source": [
    "### Code to Evaluate the Fine-Tuned DeepSeek System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e1972d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing Models and Data for DeepSeek Evaluation ---\n",
      "Using device: cuda\n",
      "Loaded and prepared 8000 training and 2000 testing samples.\n",
      "Loading fine-tuned DeepSeek model from: ./fine_tuned_deepseek_travel_prompts\n",
      "Loading FAISS index from: ./faiss_index_deepseek.idx\n",
      "Models and data ready for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Top-10: 100%|██████████| 2000/2000 [14:10<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results for Fine-Tuned DeepSeek (non-RAG) ---\n",
      "Hit Rate@10: 0.0605\n",
      "Precision@10: 0.0060\n",
      "Recall@10: 0.0605\n",
      "MAP@10: 0.0605\n",
      "NDCG@10: 0.0605\n",
      "\n",
      "--- Distribution of Hit Ranks ---\n",
      "Rank 1: 121 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ===================================================================\n",
    "# PART 1: SETUP FOR DEEPSEEK EVALUATION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Preparing Models and Data for DeepSeek Evaluation ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "ADAPTER_PATH = \"./fine_tuned_deepseek_travel_prompts\"\n",
    "FAISS_INDEX_PATH = \"./faiss_index_deepseek.idx\" # Use the DeepSeek-specific index\n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "DEEPSEEK_EMBEDDING_DIM = 1536 # Dimension for this model\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Data and Create Key Mapping ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['destination_key'] = df['destination_city'] + '__' + df['destination_country']\n",
    "df['user_profile_text'] = (df['age'].astype(str) + \" \" + df['gender'].astype(str).fillna('') + \" \" + df['hobby'].fillna('') + \" \" + df['budget'].fillna('') + \" \" + df['travel_style'].fillna(''))\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "index_to_destination_key = pd.Series(train_df['destination_key'].values, index=train_df.index)\n",
    "print(f\"Loaded and prepared {len(train_df)} training and {len(test_df)} testing samples.\")\n",
    "\n",
    "# --- Load Fine-Tuned DeepSeek Model and FAISS Index ---\n",
    "print(f\"Loading fine-tuned DeepSeek model from: {ADAPTER_PATH}\")\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, quantization_config=bnb_config, device_map={\"\": device}, trust_remote_code=True)\n",
    "embedding_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "embedding_model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loading FAISS index from: {FAISS_INDEX_PATH}\")\n",
    "faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "print(\"Models and data ready for evaluation.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PART 2: HELPER FUNCTION AND EVALUATION SCRIPT\n",
    "# ===================================================================\n",
    "\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    \"\"\"Generates an embedding from a fine-tuned PEFT model.\"\"\"\n",
    "    device = model.device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "    pooled_embedding = last_hidden_state.mean(dim=1).squeeze()\n",
    "    return pooled_embedding.cpu().numpy()\n",
    "\n",
    "# This evaluation function is the same as before\n",
    "def evaluate_retrieval_system(test_data, model, tokenizer, faiss_idx, index_map, k=10):\n",
    "    \"\"\"\n",
    "    Calculates ranking metrics and the distribution of hit ranks.\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    map_sum = 0\n",
    "    ndcg_sum = 0\n",
    "    hit_ranks = []\n",
    "    \n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=f\"Evaluating Top-{k}\"):\n",
    "        user_profile = row['user_profile_text']\n",
    "        ground_truth_item_key = row['destination_key']\n",
    "\n",
    "        user_embedding = get_embedding(user_profile, model, tokenizer).reshape(1, -1)\n",
    "        _, top_k_indices = faiss_idx.search(user_embedding, k)\n",
    "        recommended_item_keys = [index_map.get(i) for i in top_k_indices[0]]\n",
    "        \n",
    "        if ground_truth_item_key in recommended_item_keys:\n",
    "            hits += 1\n",
    "            rank = recommended_item_keys.index(ground_truth_item_key) + 1\n",
    "            hit_ranks.append(rank)\n",
    "            map_sum += 1 / rank\n",
    "            ndcg_sum += 1 / np.log2(rank + 1)\n",
    "\n",
    "    num_test_samples = len(test_data)\n",
    "    hit_rate = hits / num_test_samples\n",
    "    mean_avg_precision = map_sum / num_test_samples\n",
    "    avg_ndcg = ndcg_sum / num_test_samples\n",
    "\n",
    "    return {\n",
    "        'metrics': {\n",
    "            f'Hit Rate@{k}': hit_rate,\n",
    "            f'Precision@{k}': hit_rate / k,\n",
    "            f'Recall@{k}': hit_rate,\n",
    "            f'MAP@{k}': mean_avg_precision,\n",
    "            f'NDCG@{k}': avg_ndcg\n",
    "        },\n",
    "        'hit_rank_distribution': Counter(hit_ranks)\n",
    "    }\n",
    "\n",
    "# ===================================================================\n",
    "# PART 3: EXECUTION AND RESULTS\n",
    "# ===================================================================\n",
    "\n",
    "TOP_K = 10\n",
    "results = evaluate_retrieval_system(\n",
    "    test_data=test_df,\n",
    "    model=embedding_model,\n",
    "    tokenizer=tokenizer,\n",
    "    faiss_idx=faiss_index,\n",
    "    index_map=index_to_destination_key,\n",
    "    k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluation Results for Fine-Tuned DeepSeek (non-RAG) ---\")\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n--- Distribution of Hit Ranks ---\")\n",
    "sorted_ranks = sorted(results['hit_rank_distribution'].items())\n",
    "if not sorted_ranks:\n",
    "    print(\"No hits were recorded.\")\n",
    "for rank, count in sorted_ranks:\n",
    "    print(f\"Rank {rank}: {count} hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc104edf",
   "metadata": {},
   "source": [
    "### 1. Code to Evaluate MSE/RMSE for Fine-Tuned BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86c5d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing BERT Model for MSE/RMSE Evaluation ---\n",
      "Using device: cuda\n",
      "Loaded 2000 testing samples.\n",
      "Loading fine-tuned BERT model from: ./fine_tuned_bert_travel\n",
      "BERT model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BERT Predictions: 100%|██████████| 2000/2000 [05:21<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MSE/RMSE Results for Fine-Tuned BERT (non-RAG) ---\n",
      "MSE:  0.3879\n",
      "RMSE: 0.6228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# SETUP\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Preparing BERT Model for MSE/RMSE Evaluation ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BERT_FINETUNED_PATH = \"./fine_tuned_bert_travel\"\n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "# Simple preprocessing for consistency\n",
    "df['destination_description'] = (df['destination_city'] + \" \" + df['destination_country'] + \" \" + df['activities'].fillna('') + \" \" + df['climate'].fillna(''))\n",
    "df['user_profile_text'] = (df['age'].astype(str) + \" \" + df['gender'].astype(str).fillna('') + \" \" + df['hobby'].fillna('') + \" \" + df['budget'].fillna('') + \" \" + df['travel_style'].fillna(''))\n",
    "_ , test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Loaded {len(test_df)} testing samples.\")\n",
    "\n",
    "# --- Load Fine-Tuned BERT Model ---\n",
    "print(f\"Loading fine-tuned BERT model from: {BERT_FINETUNED_PATH}\")\n",
    "model_bert = BertForSequenceClassification.from_pretrained(BERT_FINETUNED_PATH)\n",
    "model_bert.to(device)\n",
    "model_bert.eval()\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(BERT_FINETUNED_PATH)\n",
    "print(\"BERT model loaded.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# EVALUATION\n",
    "# ===================================================================\n",
    "\n",
    "def evaluate_bert_regression(test_data, model, tokenizer):\n",
    "    true_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Evaluating BERT Predictions\"):\n",
    "        # Create the same prompt format used for training\n",
    "        prompt = (\n",
    "            f\"User is {int(row['age'])}, {row['gender']}, enjoys {row['hobby']}, \"\n",
    "            f\"budget is {row['budget']}, travel style is {row['travel_style']}. \"\n",
    "            f\"The destination is {row['destination_city']}, {row['destination_country']} offering {row['activities']} \"\n",
    "            f\"in a {row['climate']} climate.\"\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # For regression with num_labels=1, the output logit is the prediction\n",
    "        prediction = outputs.logits.item()\n",
    "        \n",
    "        true_ratings.append(row['travel_rating'])\n",
    "        predicted_ratings.append(prediction)\n",
    "\n",
    "    mse = mean_squared_error(true_ratings, predicted_ratings)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return {\"MSE\": mse, \"RMSE\": rmse}\n",
    "\n",
    "# --- Execute Evaluation ---\n",
    "bert_regression_metrics = evaluate_bert_regression(test_df, model_bert, tokenizer_bert)\n",
    "\n",
    "print(\"\\n--- MSE/RMSE Results for Fine-Tuned BERT (non-RAG) ---\")\n",
    "print(f\"MSE:  {bert_regression_metrics['MSE']:.4f}\")\n",
    "print(f\"RMSE: {bert_regression_metrics['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c372c3",
   "metadata": {},
   "source": [
    "### 2. Code to Evaluate MSE/RMSE for Fine-Tuned DeepSeek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing DeepSeek Model for MSE/RMSE Evaluation ---\n",
      "Using device: cuda\n",
      "Loaded 2000 testing samples.\n",
      "Loading fine-tuned DeepSeek model from: ./fine_tuned_deepseek_travel_prompts\n",
      "DeepSeek model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DeepSeek Predictions:  34%|███▍      | 687/2000 [33:13<1:08:59,  3.15s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# SETUP\n",
    "# ===================================================================\n",
    "print(\"\\n--- Preparing DeepSeek Model for MSE/RMSE Evaluation ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "ADAPTER_PATH = \"./fine_tuned_deepseek_travel_prompts\"\n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['destination_description'] = (df['destination_city'] + \" \" + df['destination_country'] + \" \" + df['activities'].fillna('') + \" \" + df['climate'].fillna(''))\n",
    "df['user_profile_text'] = (df['age'].astype(str) + \" \" + df['gender'].astype(str).fillna('') + \" \" + df['hobby'].fillna('') + \" \" + df['budget'].fillna('') + \" \" + df['travel_style'].fillna(''))\n",
    "_ , test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Loaded {len(test_df)} testing samples.\")\n",
    "\n",
    "# --- Load Fine-Tuned DeepSeek Model ---\n",
    "print(f\"Loading fine-tuned DeepSeek model from: {ADAPTER_PATH}\")\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, quantization_config=bnb_config, device_map={\"\": device}, trust_remote_code=True)\n",
    "model_deepseek = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model_deepseek.eval()\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)\n",
    "if tokenizer_deepseek.pad_token is None:\n",
    "    tokenizer_deepseek.pad_token = tokenizer_deepseek.eos_token\n",
    "print(\"DeepSeek model loaded.\")\n",
    "\n",
    "# ===================================================================\n",
    "# EVALUATION\n",
    "# ===================================================================\n",
    "\n",
    "def evaluate_deepseek_regression(test_data, model, tokenizer):\n",
    "    true_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Evaluating DeepSeek Predictions\"):\n",
    "        # Create the prompt format used for fine-tuning, but end before the answer\n",
    "        prompt = (\n",
    "            f\"User Profile: Age {int(row['age'])}, {row['gender']}, enjoys {row['hobby']}, \"\n",
    "            f\"budget is {row['budget']}, travel style is {row['travel_style']}.\\\\n\"\n",
    "            f\"Destination Details: City {row['destination_city']}, Country {row['destination_country']}, \"\n",
    "            f\"activities include {row['activities']}, climate is {row['climate']}.\\\\n\"\n",
    "            f\"Question: What is the travel rating for this destination by this user? Answer:\"\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate a short response, just enough for the number\n",
    "            output_tokens = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse the number from the generated text\n",
    "        match = re.search(r'Answer:\\s*(\\d+)', generated_text)\n",
    "        prediction = int(match.group(1)) if match else 3 # Default to 3 (average rating) if parsing fails\n",
    "        \n",
    "        true_ratings.append(row['travel_rating'])\n",
    "        predicted_ratings.append(prediction)\n",
    "\n",
    "    mse = mean_squared_error(true_ratings, predicted_ratings)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return {\"MSE\": mse, \"RMSE\": rmse}\n",
    "\n",
    "# --- Execute Evaluation ---\n",
    "deepseek_regression_metrics = evaluate_deepseek_regression(test_df, model_deepseek, tokenizer_deepseek)\n",
    "\n",
    "print(\"\\n--- MSE/RMSE Results for Fine-Tuned DeepSeek (non-RAG) ---\")\n",
    "print(f\"MSE:  {deepseek_regression_metrics['MSE']:.4f}\")\n",
    "print(f\"RMSE: {deepseek_regression_metrics['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4c96b",
   "metadata": {},
   "source": [
    "### 2. Code to Evaluate RAG Systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8462998",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544be37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer # Import the ROUGE scorer\n",
    "import re\n",
    "\n",
    "# ===================================================================\n",
    "# PART 1: SETUP FOR RAG EVALUATION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Preparing Models and Data for RAG Evaluation ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Models for the MiniLM-RAG System\n",
    "MINILM_FINETUNED_PATH = \"./fine_tuned_minilm_travel\"\n",
    "DEEPSEEK_BASE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# Annoy index built from the fine-tuned MiniLM model\n",
    "ANNOY_PATH_MINILM = \"./annoy_index_minilm_finetuned.ann\" \n",
    "DATA_PATH = 'travel_recommendations_10k.csv'\n",
    "MINILM_EMBEDDING_DIM = 384\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load All Necessary Models, Data, and Indexes ---\n",
    "# This script requires the same sequential loading we used before to manage memory.\n",
    "# For simplicity in this template, we will define the functions and then call them.\n",
    "# The actual loading will happen inside the main evaluation loop.\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['destination_key'] = df['destination_city'] + '__' + df['destination_country']\n",
    "df['user_profile_text'] = (df['age'].astype(str) + \" \" + df['gender'].astype(str).fillna('') + \" \" + df['hobby'].fillna('') + \" \" + df['budget'].fillna('') + \" \" + df['travel_style'].fillna(''))\n",
    "df['destination_full_description'] = (df['destination_city'] + \", \" + df['destination_country'] + \" which offers activities like \" + df['activities'].fillna(''))\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Loaded {len(test_df)} testing samples for evaluation.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PART 2: RAG PIPELINE AND EVALUATION SCRIPT\n",
    "# ===================================================================\n",
    "\n",
    "def get_embedding_from_minilm(text, model):\n",
    "    return model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "def evaluate_rag_system_with_rouge(test_data, train_data):\n",
    "    \"\"\"\n",
    "    Generates RAG recommendations and evaluates them using ROUGE.\n",
    "    \"\"\"\n",
    "    # Initialize the ROUGE scorer\n",
    "    # We will calculate ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "    scorer = rouge_scorer.Scorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Store all scores\n",
    "    all_scores = []\n",
    "\n",
    "    # --- Load Models for this evaluation run ---\n",
    "    # Load Embedding Model\n",
    "    embedding_model = SentenceTransformer(MINILM_FINETUNED_PATH).to(device)\n",
    "    annoy_index = AnnoyIndex(MINILM_EMBEDDING_DIM, 'angular')\n",
    "    annoy_index.load(ANNOY_PATH_MINILM)\n",
    "    index_to_full_desc = pd.Series(train_data['destination_full_description'].values, index=train_data.index)\n",
    "\n",
    "    # Load Generation Model\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    generation_model = AutoModelForCausalLM.from_pretrained(DEEPSEEK_BASE_MODEL_NAME, quantization_config=bnb_config, device_map={\"\": device}, trust_remote_code=True)\n",
    "    generation_model.eval()\n",
    "    tokenizer_deepseek = AutoTokenizer.from_pretrained(DEEPSEEK_BASE_MODEL_NAME, trust_remote_code=True)\n",
    "    if tokenizer_deepseek.pad_token is None:\n",
    "        tokenizer_deepseek.pad_token = tokenizer_deepseek.eos_token\n",
    "    \n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Evaluating RAG with ROUGE\"):\n",
    "        user_profile_text = row['user_profile_text']\n",
    "        \n",
    "        # 1. RETRIEVE\n",
    "        user_embedding = get_embedding_from_minilm(user_profile_text, embedding_model)\n",
    "        candidate_indices = annoy_index.get_nns_by_vector(user_embedding, 3, include_distances=False)\n",
    "        \n",
    "        # 2. AUGMENT\n",
    "        context_string = \"\"\n",
    "        for idx in candidate_indices:\n",
    "            context_string += f\"- {index_to_full_desc.get(idx, 'Unknown Destination')}\\\\n\"\n",
    "        \n",
    "        rag_prompt = (f\"A user with the profile '{user_profile_text}' is considering these destinations:\\\\n{context_string}\\\\nBased on this information, which one destination is the best recommendation? Explain why in a detailed paragraph.\")\n",
    "        \n",
    "        # 3. GENERATE\n",
    "        inputs = tokenizer_deepseek(rag_prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            output_tokens = generation_model.generate(**inputs, max_new_tokens=150, pad_token_id=tokenizer_deepseek.eos_token_id)\n",
    "        generated_text = tokenizer_deepseek.decode(output_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Clean the generated text\n",
    "        answer_marker = \"Explain why in a detailed paragraph.\"\n",
    "        answer_start_index = generated_text.find(answer_marker)\n",
    "        if answer_start_index != -1:\n",
    "            generated_recommendation = generated_text[answer_start_index + len(answer_marker):].strip()\n",
    "        else:\n",
    "            generated_recommendation = \"\" # If no answer, it's a poor generation\n",
    "\n",
    "        # 4. CREATE REFERENCE & EVALUATE\n",
    "        # Create a simple \"ground truth\" reference answer\n",
    "        reference_answer = f\"A good recommendation for this user is {row['destination_full_description']}.\"\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(reference_answer, generated_recommendation)\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    # --- Calculate Average ROUGE scores ---\n",
    "    avg_rouge1 = np.mean([s['rouge1'].fmeasure for s in all_scores])\n",
    "    avg_rouge2 = np.mean([s['rouge2'].fmeasure for s in all_scores])\n",
    "    avg_rougeL = np.mean([s['rougeL'].fmeasure for s in all_scores])\n",
    "    \n",
    "    return {\n",
    "        'ROUGE-1': avg_rouge1,\n",
    "        'ROUGE-2': avg_rouge2,\n",
    "        'ROUGE-L': avg_rougeL # 'L' stands for Longest Common Subsequence\n",
    "    }\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PART 3: EXECUTION\n",
    "# ===================================================================\n",
    "\n",
    "# Note: This process will be slow as it generates text for every sample.\n",
    "# For a quicker test, you can use a smaller subset of the test data:\n",
    "# test_df_sample = test_df.head(100)\n",
    "rag_metrics = evaluate_rag_system_with_rouge(test_df, train_df)\n",
    "\n",
    "print(\"\\n--- ROUGE Evaluation Results for Fine-Tuned MiniLM-RAG ---\")\n",
    "for metric, value in rag_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel_recommender_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
